\chapter{Running a \CmdStan Program}\label{stan-cmd.chapter}

\noindent
Once a \CmdStan program is compiled, it can be run in many different
ways.  It can be used to sample or optimize parameters, or to diagnose
a model.  Before diving into the detailed configurations, the first
section provides some simple examples.


\section{Getting Started by Example}\label{command-getting-started.section}

Once a \CmdStan program has been converted to a \Cpp program for that
model (see \refchapter{stanc}) and the resulting \Cpp program compiled
to a platform-specific executable (see \refchapter{compiling}),
the model is ready to be run.

All of the \CmdStan functionality is highly configurable from the command
line; the options are defined later in this chapter.  Each command
option also has defaults, which are used in this section.

\subsection{Sampling}

Suppose the executable is in file \code{my\_model} and the data is in
file \code{my\_data}, both in the current working directory.  To
generate samples from a data set using the default settings, use one
of the following, depending on platform.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model sample data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model sample data file=my_data
\end{Verbatim}
\end{quote}
%
On both platforms, this command reads the data from file
\code{my\_data}, runs warmup tuning for 1000 iterations (the values of
which are discarded), and then runs the fully-adaptive \NUTS sampler
for 1000 iterations, writing the parameter (and other) values to the
file \code{samples.csv} in the current working directory.  When no
random number seed is specified, a seed is generated from the system
time.

\subsection{Sampling in Parallel}

The previous example executes one chain, which can be repeated to
generate multiple chains. However, users may want to execute chains
in parallel on a multicore machine.

\subsubsection{Mac OS and Linux}

To sample four chains using a Bash shell on Mac OS or Linux, execute%
%
\footnote{Complicated multiline commands such as this one are prime candidates for putting into a script file.}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> for i in {1..4}
  do
    ./my_model sample random seed=12345 \
       id=$i data file=my_data \
       output file=samples$i.csv &
 done
\end{Verbatim}
\end{quote}
%
The ampersand (\code{\&}) at the end of the nested command pushes each process into the background, so that the loop can continue without waiting for the current chain to finish.  The \code{id} value makes sure that a non-overlapping set of random numbers are used for each chain.  Also note that the output file is explicitly specified, with the variable \code{\$i} being used to ensure the output file name for each chain is unique.

The terminal standard output will be interleaved for all chains
running concurrently.  To suppress all terminal output, direct the
standard output to the ``null'' device.  This is achieved by
postfixing \code{> /dev/null} to a command, which in the above case,
means changing the second-to-last line to
\begin{quote}
\begin{Verbatim}
       output file=samples$i.csv > /dev/null &
\end{Verbatim}
\end{quote}



\subsubsection{Windows}

On Windows, the following is functionally equivalent to the Bash
snippet above
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> for /l %x in (1, 1, 4) do start /b model  sample   ^
   random seed=12345 id=%x data file=my_data         ^
   output file=samples%x.csv
\end{Verbatim}
\end{quote}
%
The caret (\code{\textasciicircum}) indicates a line continuation in
DOS.

\subsubsection{Combining Parallel Chains}

\CmdStan has commands to analyze the output of multiple chains, each
stored in their own file; see \refchapter{stansummary}.  RStan also
has commands to read in multiple CSV files produced by \CmdStan's
command-line sampler.

To compute posterior quantities, it is sometimes easier to have the
chains merged into a single CSV file.  If the grep and sed programs
are installed, then the following will combine the four
comma-separated values files into a single comma-separated values
file.  The command is the same on Windows, Mac OS, and Linux.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> grep lp__ samples1.csv > combined.csv
> sed '/^[#l]/d' samples*.csv >> combined.csv
\end{Verbatim}
\end{quote}

\subsubsection{Scripting and Batching}

The previous examples show how to sample in parallel from the command
line.  Operations like these can also be scripted, using shell scripts
(\code{.sh}) on Mac OS and Linux and DOS batch (\code{.bat}) files on
Windows.  A sequence of several such commands can be executed from a
single script file.  Such scripts might contain \code{stanc} commands
(see \refchapter{stanc}) and \code{stansummary} commands (see
\refchapter{stansummary}) can be executed from a single script file.
At some point, it is worthwhile to move to something with stronger
dependency control such as makefiles.


\subsection{Optimization}

\CmdStan can find the posterior mode (assuming there is one).  If the
posterior is not convex, there is no guarantee Stan will be able to
find the global mode as opposed to a local optimum of log probability.

For optimization, the mode is calculated without the Jacobian
adjustment for constrained variables, which shifts the mode due to the
change of variables.  Thus modes correspond to modes of the model as
written.

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model optimize data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model optimize data file=my_data
\end{Verbatim}
\end{quote}


\subsection{Variational Inference}

\CmdStan can fit a variational approximation to the posterior. The approximation
is a Gaussian in the unconstrained variable space. \Stan implements two
variational algorithms. The \code{algorithm=meanfield} option uses a fully
factorized Gaussian for the approximation. The \code{algorithm=fullrank} option
uses a Gaussian with a full-rank covariance matrix for the approximation.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model variational algorithm=meanfield  \
             data file=my_data
> ./my_model variational algorithm=fullrank   \
             data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model variational algorithm=meanfield    \
           data file=my_data
> my_model variational algorithm=fullrank     \
           data file=my_data
\end{Verbatim}
\end{quote}

\section{Diagnostics}\label{diagnostics.section}

\CmdStan has a basic diagnostic feature that will calculate gradients of
the initial state and compare them with those calculated with finite
differences.  If there are discrepancies, there is a problem with the
model or initial states (or a bug in \Stan).  To run on the different
platforms, use one of the following.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model diagnose data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model diagnose data file=my_data
\end{Verbatim}
\end{quote}




\section{Command-Line Options}\label{stan-command-line-options.section}

\CmdStan executables are highly configurable, allowing the user to specify
and customize not only the calculation method but also the data, output,
initialization, and random number generation.  The arguments are defined
hierarchically so that, for example, optimization settings are not necessary
when sampling.

The atomic elements of the hierarchy (i.e., those without
corresponding values) are \textit{categorical arguments} (sometimes
called ``flags'') which define self-contained categories of arguments.

\CmdStan's commands have more hierarchical structure than is typical of
command line executables, which usually have at most two subgroups of
commands.  Arguments grouped within a category are not ordered with
respect to each other.  The only ordering is that the global options
come before the method argument and subcommand-specific options after
the method argument.  For example, the following four commands all
define the same configuration:%
%
\footnote{
The backslash (\code{\textbackslash}) is used at the end of a line in
a command to indicate that it continues on the next line.  The
indentation to indicate the structure of the command is for
pedagogical purposes only; the same result would be obtained writing
each command on one line with single spaces separating the elements.
}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model sample output file=samples.csv                \
                        diagnostic_file=diagnostics.csv \
                 random seed=1

> ./model sample output diagnostic_file=diagnostics.csv \
                        file=samples.csv                \
                 random seed=1

> ./model sample random seed=1                          \
                 output file=samples.csv                \
                        diagnostic_file=diagnostics.csv

> ./model sample random seed=1                          \
                 output diagnostic_file=diagnostics.csv \
                        file=samples.csv
\end{Verbatim}
\end{quote}
%
The categorical arguments \code{output} and \code{random} can be
in any order provided that the subarguments follow their respective
parent, here \code{diagnostic\_file} and \code{file} following \code{output}
and \code{seed} coming after \code{random}.  These four configurations
exhaust all valid combinations.

Categorical arguments may appear is isolation, for example when
introducing \code{sample} or \code{random}, or they may appear as
the values for other arguments, such as \code{hmc} which not only
introduces a category of HMC related arguments but also defines the
value of the argument \code{algorithm}.  A visual diagram of the available
categorical arguments is shown in \reffigure{hierarchy}, with the mutual
exclusivity of these arguments as values shown in \reffigure{configuration}.
Specifying conflicting arguments
causes the execution to immediately terminate.

\begin{figure}
\setlength{\unitlength}{0.01in}
\centering
\begin{picture}(480, 480)
\small\tt
%
\put(10, 460) { \makebox(50, 10)[l]{id, data, init} }
%
\put(0, 410) { \framebox(470, 45)\hss }
\put(10, 440) { \makebox(50, 10)[l]{random} }
\put(50, 420) { \makebox(50, 10)[l]{seed} }
%
\put(0, 360) { \framebox(470, 45)\hss }
\put(10, 390) { \makebox(65, 10)[l]{output} }
\put(50, 370) { \makebox(500, 10)[l]{file, diagnostic\_file, ... } } % refresh, save\_warmup} }
%
\put(10, 340) { \makebox(50, 10)[l]{method} }
\put(20, 117.5) { \line(0, 1){212.5} }
\put(20, 117.5) { \vector(1, 0){15} }
\put(20, 257.5) { \vector(1, 0){15} }
\put(20, 307.5) { \vector(1, 0){15} }
%
\put(40, 285) { \framebox(430, 45)\hss }
\put(50, 315) { \makebox(65, 10)[l]{ diagnose } }
\put(90, 295) { \makebox(50, 10)[l]{ $\ldots$ } }
%
\put(40, 235) { \framebox(430, 45)\hss }
\put(50, 265) { \makebox(63, 10)[l]{ optimize } }
\put(90, 245) { \makebox(50, 10)[l]{ $\ldots$ } }
%
\put(40, 5) { \framebox(430, 225)\hss }
\put(50, 215) { \makebox(55, 10)[l]{sample} }
\put(90, 195) { \makebox(350, 10)[l]{num\_samples, num\_warmup, save\_warmup, thin} }
%
\put(80, 140) { \framebox(380, 45)\hss }
\put(90, 170) { \makebox(55, 10)[l]{adapt} }
\put(130, 150) { \makebox(50, 10)[l]{$\ldots$} }
%
\put(90, 120) { \makebox(55, 10)[l]{algorithm} }
\put(100, 37.5) { \line(0, 1){70} }
\put(100, 87.5) { \vector(1, 0){15} }
\put(100, 37.5) { \vector(1, 0){15} }
%
\put(120, 65) { \framebox(340, 45)\hss }
\put(130, 95) { \makebox(55, 10)[l]{hmc} }
\put(170, 75) { \makebox(50, 10)[l]{$\ldots$} }
%
\put(120, 15) { \framebox(340, 45)\hss }
\put(130, 45) { \makebox(115, 10)[l]{nuts} }
\put(170, 25) { \makebox(50, 10)[l]{ $\ldots$ } }
\end{picture}
\caption{\small\it In the hierarchical argument structure, certain
  arguments, such as \code{random} and \code{output}, introduce new
  categories of arguments.  Categorical arguments may also
  appear as values of other arguments, such as \code{diagnose},
  \code{optimize}, and \code{sample}, which define the mutually
  exclusive values for the argument \code{method}. }%
\label{hierarchy.figure}
\end{figure}

\begin{figure}
\setlength{\unitlength}{0.01in}
%\centering
\begin{picture}(480, 480)
%
\small\tt
\put(10, 460) { \makebox(50, 10)[l]{id, data, init} }
%
\put(0, 410){ \framebox(470, 45) \hss}
\put(10, 440) { \makebox(50, 10)[l]{random} }
\put(50, 420) { \makebox(50, 10)[l]{seed} }
%
\put(0, 360) { \framebox(470, 45)\hss }
\put(10, 390) { \makebox(65, 10)[l]{output} }
\put(50, 370) { \makebox(450, 10)[l]{file, diagnostic\_file, ...} }  % refresh, ...
%
\put(10, 340) { \makebox(50, 10)[l]{method} }
\put(20, 117.5) { \line(0, 1){212.5} }
\put(20, 117.5) { \vector(1, 0){15} }
\put(20, 257.5) { \color{gray!30}\vector(1, 0){15} }
\put(20, 307.5) { \color{gray!30}\vector(1, 0){15} }
%
\put(40, 285) { \color{gray!30}\framebox(430, 45)\hss }
\put(50, 315) { \makebox(65, 10)[l]{ \textcolor{gray!30}{diagnose} } }
\put(90, 295) { \makebox(50, 10)[l]{ \textcolor{gray!30}{$\ldots$} } }
%
\put(40, 235) { \color{gray!30}\framebox(430, 45)\hss }
\put(50, 265) { \makebox(63, 10)[l]{ \textcolor{gray!30}{optimize} } }
\put(90, 245) { \makebox(50, 10)[l]{ \textcolor{gray!30}{$\ldots$} } }
%
\put(40, 5) { \framebox(430, 225)\hss }
\put(50, 215) { \makebox(55, 10)[l]{sample} }
\put(90, 195) { \makebox(350, 10)[l]{num\_samples, num\_warmup, save\_warmup, thin} }
%
\put(80, 140) { \framebox(370, 45)\hss }
\put(90, 170) { \makebox(55, 10)[l]{adapt} }
\put(130, 150) { \makebox(50, 10)[l]{$\ldots$} }
%
\put(90, 120) { \makebox(55, 10)[l]{algorithm} }
\put(100, 37.5) { \color{gray!30}\line(0, 1){70} }
\put(100, 87.5) { \line(0, 1){20} }
\put(100, 87.5) { \vector(1, 0){15} }
\put(100, 37.5) { \color{gray!30}\vector(1, 0){15} }
%
\put(120, 65) { \framebox(330, 45)\hss }
\put(130, 95) { \makebox(55, 10)[l]{hmc} }
\put(170, 75) { \makebox(50, 10)[l]{$\ldots$} }
%
\put(120, 15){ \color{gray!30}\framebox(330, 45)\hss }
\put(130, 45) { \makebox(95, 10)[l]{ \textcolor{gray!30}{nuts} } }
\put(170, 25) { \makebox(50, 10)[l]{ \textcolor{gray!30}{$\ldots$} } }
\end{picture}
\caption{\small\it A valid argument configuration defines only one
    mutually exclusive argument.  If conflicting arguments are
    specified, for example \code{method=optimize method=sample}, then
    execution immediately terminates with a warning
    message.}\label{configuration.figure}
\end{figure}

Note that any valid argument configuration must either specify a method
or a help request.

\subsection{Method}

All commands other than \code{help} must include at least one
method, specified explicitly as \code{method=\farg{method\_name}} or
implicitly with only \farg{method\_name}.  Currently \CmdStan supports
the following methods:
%
\begin{center}
\begin{tabular}{r|l}
{\it Method} & {\it Description} \\ \hline \hline
{\tt sample}   &  sample using MCMC
\\
{\tt optimize} &  find posterior mode using optimization
\\
{\tt variational} &  fit variational approximation (experimental)
\\
{\tt diagnose} &  diagnose models
\end{tabular}
\end{center}
%
All remaining configurations are optional, with default values
provided for all arguments not explicitly specified.

\subsection{Help}

Informative output can be retrieved either globally, by requesting help
at the top-level, or locally, by requesting help deeper into the hierarchy.
Note that after any help has been displayed the execution immediately
terminates, even if a method has been specified.

\subsubsection{Top-Level Help}

If \code{help} is specified as the only argument then a usage message is
displayed.  Similarly, specifying \code{help\_all} by itself displays the entire
argument hierarchy.

\subsubsection{Context-Sensitive Help}

Specifying \code{help} after any argument displays a description and
valid options for that argument.  For example,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
./my_model sample help
\end{Verbatim}
\end{quote}
%
provides the top-level options for the \code{sample} method.

Detailed information on the argument, and all arguments deriving
from it, can accessed by specifying \code{help-all} instead,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
./my_model sample help-all
\end{Verbatim}
\end{quote}

\section{Full Argument Hierarchy}\label{detailed-command-arguments.section}

Here we present the full argument hierarchy, along with relevant details.
Some typical use-case examples are provided in the next section.

\subsection{Typographical Conventions}

The following typographical conventions are obeyed in the hierarchy.
%
\begin{itemize}
\item \code{arg=\farg{<value-type>}}
\\
Arguments with values; displays the value type, legal values, and default value
%
\item \code{\bfseries arg}
\\
Isolated categorical arguments; displays all valid subarguments
%
\item \code{\farg{value}}
\\
Values; describes effect of selecting the value
%
\item \code{\farg{\bfseries avalue}}
\\
Categorical arguments that appear as values to other arguments; displays all valid subarguments
\end{itemize}

\subsection{Top-Level Method Argument}

Every command must have exactly one method specified as the very first
argument.  The value type of \code{list element} means that the valid
values are enumerated as a list.

\begin{description}
\hiercmdarg{}{method}{$$list element$$}
  {Analysis method (Note that \code{method=} is optional)}
  {Valid values: \  \code{sample, optimize, variational, diagnose}}
  {Defaults to \code{sample}}
\end{description}

\subsection{Sampling-Specific Arguments}

The following arguments are specific to sampling.  The method argument
\code{sample} (or \code{method=sample}) must come first in order to
enable the subsequent arguments.  The other arguments are optional and
may appear in any order.

\begin{description}
  \hierlongcmd{\indentarrow}{\farg{\bfseries sample}}
    {Bayesian inference with Markov Chain Monte Carlo}
    {Valid subarguments:
      \code{num\_samples, num\_warmup, save\_warmup,
        \\ \hspace*{24pt} thin, adapt, algorithm}}
%
    \hiercmdarg{\indentarrow\indentarrow}{num\_samples}{$$int$$}
      {Number of sampling iterations}
      {Valid values: \  $0 \leq \mbox{\code{num\_samples}}$}
      {Defaults to \code{1000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{num\_warmup}{$$int$$}
      {Number of warmup iterations}
      {Valid values: \  $0 \leq \mbox{\code{warmup}}$}
      {Defaults to \code{1000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{save\_warmup}{$$boolean$$}
      {Stream warmup samples to output?}
      {Valid values: \ \code{0, 1}}
      {Defaults to \code{0}}
%
    \hiercmdarg{\indentarrow\indentarrow}{thin}{$$int$$}
      {Period between saved samples}
      {Valid values: \  $0 < \mbox{\code{thin}}$}
      {Defaults to \code{1}}
%
\end{description}

\subsubsection{Sampling Adaptation-Specific Parameters}

\begin{figure}
\setlength{\unitlength}{0.005in}
\centering
\begin{picture}(1000, 200)
%
\footnotesize
\put(25, 20) { \framebox(75, 200)[c]{I} }
\put(100, 20) { \framebox(25, 200)[c]{II} }
\put(125, 20) { \framebox(50, 200)[c]{II} }
\put(175, 20) { \framebox(100, 200)[c]{II} }
\put(275, 20) { \framebox(200, 200)[c]{II} }
\put(475, 20) { \framebox(400, 200)[c]{II} }
\put(875, 20) { \framebox(50, 200)[c]{III} }
\put(25, 20) { \vector(1, 0){950} }
\put(800, -10) { \makebox(200, 20)[l]{{\small Iteration}} }
%
\end{picture}
\caption{\small\it Adaptation during warmup occurs in three stages: an initial
fast adaptation interval (I), a series of expanding slow adaptation intervals (II),
and a final fast adaptation interval (III).  For HMC, both the fast and slow intervals
are used for adapting the step size, while the slow intervals are used for learning
the (co)variance necessitated by the metric.  Iteration numbering
starts at 1 on the left side of the figure and increases to the right.}%
\label{adaptation.figure}
\end{figure}

When adaptation is engaged the warmup period is split into three
stages (Figure \ref{adaptation.figure}), with two \textit{fast} intervals
surrounding a series of growing \textit{slow} intervals.  Here fast
and slow refer to parameters that adapt using local and global
information, respectively; the Hamiltonian Monte Carlo samplers,
for example, define the step size as a fast parameter and
the (co)variance as a slow parameter.  The size of the the initial
and final fast intervals and the initial size of the slow interval
are all customizable, although user-specified values may be modified
slightly in order to ensure alignment with the warmup period.

The motivation behind this partitioning of the warmup period is to
allow for more robust adaptation.  In the initial fast interval the
chain is allowed to converge towards the typical set,%
%
\footnote{The typical set is a concept borrowed from information
theory and refers to the neighborhood (or neighborhoods in multimodal models)
of significant posterior probability mass through which the Markov chain
will travel in equilibrium.}
%
with only parameters that can learn from local information adapted.
After this initial stage parameters that require global information,
for example (co)variances, are estimated in a series of expanding,
memoryless windows; often fast parameters will be adapted here
as well.  Lastly the fast parameters are allowed to adapt to the final
update of the slow parameters.

Currently all \Stan sampling algorithms utilize dual averaging to
optimize the step size (this optimization during adaptation of the
sampler should not be confused with running \Stan's optimization method).
This optimization procedure is extremely flexible and for completeness
we have exposed each option, using the notation of
\citep{Hoffman-Gelman:2011, Hoffman-Gelman:2014}.  In practice the
efficacy of the optimization is sensitive to the value of these
parameters, and we do not recommend changing the defaults without
experience with the dual averaging algorithm.  For more information,
see the discussion of dual averaging in \citep{Hoffman-Gelman:2011,
 Hoffman-Gelman:2014}.

Variances or covariances are estimated using Welford accumulators
to avoid a loss of precision over many floating point operations.

The following subarguments are introduced by the categorical argument
\code{adapt}.  Each subargument must contiguously follow \code{adapt},
though they may appear in any order.

\begin{description}
    \hierlongcmd{\indentarrow\indentarrow}{\bfseries adapt}
      {Warmup Adaptation}
      {Valid subarguments: \code{engaged, gamma, delta, kappa, t0}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{engaged}{$$boolean$$}
        {Adaptation engaged?}
        {Valid values: \ \code{0, 1}}
        {Defaults to \code{1}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{gamma}{$$double$$}
        {Adaptation regularization scale}
        {Valid values: \  $0 < \mbox{\code{gamma}}$}
        {Defaults to \code{0.05}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{delta}{$$double$$}
        {Adaptation target acceptance statistic}
        {Valid values: \  $0 < \mbox{\code{delta}} < 1$}
        {Defaults to \code{0.8}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{kappa}{$$double$$}
        {Adaptation relaxation exponent}
        {Valid values: \  $0 < \mbox{\tt kappa}$}
        {Defaults to \code{0.75}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{t0}{$$double$$}
        {Adaptation iteration offset}
        {Valid values: \  $0 < \mbox{\code{t0}}$}
        {Defaults to \code{10}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{init\_buffer}{$$unsigned int$$}
        {Width of initial fast adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{75}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{term\_buffer}{$$unsigned int$$}
        {Width of final fast adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{50}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{window}{$$unsigned int$$}
        {Initial width of slow adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{25}}
%
\end{description}
%
By setting the acceptance statistic \code{delta} to a value closer to
1 (its value must be strictly less than 1 and its default value is
0.8), adaptation will be forced to use smaller step sizes.  This can
improve sampling efficiency (effective samples per iteration) at the
cost of increased iteration times.  Raising the value of \code{delta}
will also allow some models that would otherwise get stuck overcome
their blockages; see also the \code{stepsize\_jitter} argument.

\subsubsection{Sampling Algorithm- and Engine-Specific Arguments}

The following batch of arguments are used to control the sampler used
for sampling.  The top-level specification is for engine, the only
valid value of which is \code{hmc} (this will change in the future as
we add new samplers).

\begin{description}
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Sampling algorithm}
      {Valid values: \  \code{hmc}, \code{fixed\_param} }
      {Defaults to \code{hmc}}
\end{description}
%
Hamiltonian Monte Carlo is a very general approach to sampling that
utilizes techniques of differential geometry and mathematical physics
to generate efficient MCMC transitions.  This generality manifests in
a wealth of implementation choices.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries hmc}}
        {Hamiltonian Monte Carlo}
        {Valid subarguments: \code{engine, metric, stepsize, stepsize\_jitter}}
\end{description}
%
All HMC implementations require at least two parameters: an
integration step size and a total integration time.  We refer to
different specifications of the latter as \textit{engines}.

In the \code{static\_hmc} implementation the total integration time
must be specified by the user, where as the \code{nuts}
implementation uses the No-U-Turn Sampler to determine
an optimal integration time dynamically.
%
\begin{description}
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{engine}{$$list element$$}
          {Engine for Hamiltonian Monte Carlo}
          {Valid values: \  \code{static, nuts}}
          {Defaults to \code{nuts}}
\end{description}
%
The following options are activated for static HMC.
%
\begin{description}
          \hierlongcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{\bfseries static}}
            {Static integration time}
            {Valid subarguments: \code{int\_time}}
%
            \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{int\_time}{$$double$$}
              {Total integration time for Hamiltonian evolution}
              {Valid values: \  $0 < \mbox{\code{int\_time}}$}
              {Defaults to $2\pi$}
\end{description}
%
These options are for NUTS, an adaptive version of HMC.
%
\begin{description}
          \hierlongcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{\bfseries nuts}}
            {The No-U-Turn Sampler}
            {Valid subarguments: \code{max\_depth}}
%
\end{description}

\subsubsection{Tree Depth}

NUTS generates a proposal by evolving the initial system both forwards
and backwards in time to form a balanced binary tree.  At each iteration
of the NUTS algorithm the tree depth is increased by one, doubling
the number of leapfrog steps and effectively doubles the computation
time.  The algorithm terminates in one of two ways: either the NUTS
criterion is satisfied for a new subtree or the completed tree, or the
depth of the completed tree hits \code{max\_depth}.

Both the tree depth and the actual number of leapfrog steps computed are
reported along with the parameters in the output as \code{treedepth\_\_} and
\code{n\_leapfrog\_\_}, respectively.  Because the final subtree may only
be partially constructed, these two will always satisfy
%
\begin{equation*}
2^{\mathrm{treedepth} - 1} - 1 < N_{\mathrm{leapfrog}} \le 2^{\mathrm{treedepth} } - 1.
\end{equation*}

\code{treedepth\_\_} is an important diagnostic tool for NUTS.  For example,
\code{treedepth\_\_ = 0} occurs when the first leapfrog step is immediately
rejected and the initial state returned, indicating extreme curvature and
poorly-chosen step size (at least relative to the current position).  On the other
hand, if \code{treedepth\_\_ = max\_depth} then NUTS is taking many leapfrog
steps and being terminated prematurely to avoid excessively long execution
time.  For the most efficient sampling \code{max\_depth} should be increased
to ensure that the NUTS tree can grow as large as necessary.

For more information on the NUTS algorithm see \citep{Hoffman-Gelman:2011, Hoffman-Gelman:2014}.

\begin{description}
            \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{max\_depth}{$$int$$}
              {Maximum tree depth}
              {Valid values: \  $0 < \mbox{\code{max\_depth}}$}
              {Defaults to \code{10}}
%
\end{description}
%



\subsubsection{Euclidean Metric}

All HMC implementations in \Stan utilize quadratic kinetic energy
functions which are specified up to the choice of a symmetric,
positive-definite matrix known as a \textit{mass matrix} or, more
formally, a \textit{metric} \citep{Betancourt-Stein:2011}.

If the metric is constant then the resulting implementation is known
as \textit{Euclidean} HMC.  \Stan allows for three Euclidean HMC
implementations: a unit metric, a diagonal metric, and a dense
metric.  These can be specified with the values \code{unit\_e},
\code{diag\_e}, and \code{dense\_e}, respectively.

Future versions of \Stan will also include dynamic metrics associated
with \textit{Riemannian} HMC \citep{GirolamiCalderhead:2011, Betancourt:2012}.
%
\begin{description}
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{metric}{$$list element$$}
          {Geometry of base manifold}
          {Valid values: \  \code{unit\_e, diag\_e, dense\_e}}
          {Defaults to \code{diag\_e}}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{unit\_e}}
            {Euclidean manifold with unit metric}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{diag\_e}}
            {Euclidean manifold with diag metric}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{dense\_e}}
            {Euclidean manifold with dense metric}
\end{description}
%

\subsubsection{Step Size and Jitter}

All implementations of HMC also use numerical integrators requiring a
step size.  We also allow that step size to be ``jittered'' randomly
during sampling to avoid any poor interactions with a fixed step size
and regions of high curvature.  The maximum amount of jitter is 1,
which will cause step sizes to be selected in the range of 0 to twice
the adapted step size.  Low step sizes can get HMC samplers unstuck
that would otherwise get stuck with higher step sizes.  The downside
is that jittering below the adapted value will increase the number of
leapfrog steps required and thus slow down iterations, whereas
jittering above the adapted value can cause premature rejection due to
simulation error in the Hamiltonian dynamics calculation.  See
\citep{Neal:2011} for further discussion of step-size jittering.
%
\begin{description}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{stepsize}{$$double$$}
          {Step size for discrete evolution}
          {Valid values: \  $0 < \code{stepsize}$}
          {Defaults to \code{1}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{stepsize\_jitter}{$$double$$}
          {Uniformly random jitter of the stepsize, in percent}
          {Valid values: \  $0 \leq \mbox{\code{stepsize\_jitter}} \leq 1$}
          {Defaults to \code{0}}
\end{description}

\subsubsection{Fixed Parameter Sampler}

The fixed parameter sampler generates a new sample without changing
the current state of the Markov chain; only generated quantities may
change.  This can be useful when, for example, trying to generate pseudo-data
using the generated quantities block. If the parameters block is empty (no parameters) then using {\tt algorithm=fixed\_param} is mandatory.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries fixed\_param}}
        {Fixed Parameter Sampler}
\end{description}

\subsection{Optimization-Specific Commands}

The following arguments are for the top-level method \code{optimize}.
They allow control of the optimization algorithm, and some of its
configuration.  The other arguments may appear in any order.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries optimize}}
    {Point estimation}
    {Valid subarguments: \code{algorithm, iter, save\_iterations}}
%
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Optimization algorithm}
      {Valid values: \  \code{bfgs, lbfgs, newton}}
      {Defaults to \code{lbfgs}}
\end{description}
%
The following options are for the (L-)BFGS optimizer. L-BFGS is the default
optimizer and also much faster than the other optimizers.

Convergence monitoring in (L-)BFGS is controlled by a number of tolerance
values, any one of which being satisified causes the algorithm to
terminate with a solution.
%
\begin{itemize}
\item The log probability is considered to have converged if
\[
\left| \log p(\theta_{i}|y) - \log p(\theta_{i-1}|y) \right| <
\mbox{\code{tol\_obj}}
\]
or
\[
\frac{\left| \log p(\theta_{i}|y) - \log p(\theta_{i-1}|y) \right|}{\
\max\left(\left| \log p(\theta_{i}|y)\right|,\left| \log p(\theta_{i-1}|y)\right|,1.0\right)}
 < \mbox{\code{tol\_rel\_obj}} * \epsilon.
\]
\item The parameters are considered to have converged if
%
\\
\[
|| \theta_{i} - \theta_{i-1} || < \mbox{\code{tol\_param}}.
\]
%
\item The gradient is considered to have converged to 0 if
\[
|| g_{i} || < \mbox{\code{tol\_grad}}
\]
or
\[
\frac{g_{i}^T \hat{H}_{i}^{-1} g_{i} }{ \max\left(\left|\log p(\theta_{i}|y)\right|,1.0\right) } < \mbox{\code{tol\_rel\_grad}} * \epsilon.
\]
\end{itemize}
%
Here, $i$ is the current iteration, $\theta_{i}$ is the value of the
parameters at iteration $i$, $y$ is the data, $p(\theta_{i}|y)$ is
the posterior probability of $\theta_{i}$ up to a proportion,
$\nabla_{\theta}$ is the gradient operator with respect to $\theta$,
$g_{i} = \nabla_{\theta} \log p(\theta_{i}|y)$ is the gradient at
iteration $i$, $\hat{H}_{i}$ is the estimate of the Hessian at
iteration $i$, $|u|$ is absolute value (L1 norm) of $u$,
$||u||$ is vector length (L2 norm) of $u$, and $\epsilon \approx 2e-16$ is
machine precision.  Any of the convergence tests can be disabled
by setting its corresponding tolerance parameter to zero.

The other command-line argument for (L-)BFGS is \code{init\_alpha},
which is the first step size to try on the initial iteration. If the
first iteration takes a long time (and requires a lot of function
evaluations), set \code{init\_alpha} to be the roughly equal to the
alpha used in that first iteration.  \code{init\_alpha} has a tiny
default value, which is reasonable for many problems but might be too
large or too small depending on the objective function and
initialization. Being too big or too small just means that the first
iteration will take longer (i.e., require more gradient evaluations)
before the line search finds a good step length. It's not a critical
parameter, but for optimizing the same model multiple times (as
you tweak things or with different data) being able to change it can
save some real time.

Finally, L-BFGS has a additional command-line argument, \code{history\_size},
which controls how much memory is used maintaining the approximation of
the Hessian.  This should be less than the dimensionality of the parameter
space and, in general, relatively small values (5 - 10) are sufficient.
If L-BFGS performs badly but BFGS is performing well, then consider increasing
this.  Note that increasing this will increase the memory usage, although
this is unlikely to be an issue for typical Stan models.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries (l)bfgs}}
        {(L-)BFGS with linesearch}
        {Valid subarguments: \code{init\_alpha}, \code{tol\_obj}, \code{tol\_rel\_obj}, \code{tol\_grad}, \code{tol\_rel\_grad}, \code{tol\_param}, \code{history\_size} (lbfgs only)}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{init\_alpha}{$$double$$}
           {Line search step size for first iteration}
           {Valid values: $0 \leq \mbox{\code{init\_alpha}}$}
           {Defaults to \code{0.001}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_obj}{$$double$$}
           {Convergence tolerance on changes in objective function value}
           {Valid values: $0 \leq \mbox{\code{tol\_obj}}$}
           {Defaults to \code{1e-12}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_rel\_obj}{$$double$$}
           {Convergence tolerance on relative changes in objective function value}
           {Valid values: $0 \leq \mbox{\code{tol\_rel\_obj}}$}
           {Defaults to \code{1e+4}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_grad}{$$double$$}
        {Convergence tolerance on the norm of the gradient}
        {Valid values: $0 \leq \mbox{\code{tol\_grad}}$}
        {Defaults to \code{1e-8}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_rel\_grad}{$$double$$}
        {Convergence tolerance on the relative norm of the gradient}
        {Valid values: $0 \leq \mbox{\code{tol\_rel\_grad}}$}
        {Defaults to \code{1e+7}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_param}{$$double$$}
        {Convergence tolerance on changes in parameter value}
        {Valid values: $0 \leq \mbox{\code{tol\_param}}$}
        {Defaults to \code{1e-8}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{history\_size}{$$int$$}
           {Number of update vectors to use in Hessian approximations (lbfgs only)}
           {Valid values: $0 < \mbox{\code{history\_size}}$}
           {Defaults to \code{5}}
%
\end{description}
%
The following argument is for Newton's optimization method;  there are
currently no configuration parameters for Newton's method, and it is
not recommended because of the slow Hessian calculation involving
finite differences.
%
\begin{description}
      \hiershortcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries
          newton}}
        {Newton's method}
%
\end{description}
%
The remaining arguments apply to all optimizers.
\begin{description}
%
    \hiercmdarg{\indentarrow\indentarrow}{iter}{$$int$$}
      {Total number of iterations}
      {Valid values: \  $0 < \mbox{\code{iter}}$}
      {Defaults to \code{2000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{save\_iterations}{$$boolean$$}
      {Stream optimization progress to output?}
      {Valid values: \  \ \code{0, 1}}
      {Defaults to \code{0}}
%
\end{description}

\subsection{Variational Inference-Specific Commands}

The following arguments are for the top-level method \code
{variational}.
They allow control of the variational inference algorithm, and some of its
configuration.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries variational}}
    {Variational inference}
    {Valid subarguments: \code{algorithm, grad\_samples, elbo\_samples,
    eta\_adagrad, iter, tol\_rel\_obj, eval\_elbo, output\_samples}}
%
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Variational inference algorithm}
      {Valid values: \ \code{meanfield, fullrank}}
      {Defaults to \code{meanfield}}
%
    \hiercmdarg{\indentarrow\indentarrow}{grad\_samples}{$$int$$}
      {Number of samples for Monte Carlo estimate of gradients}
      {Valid values: \ $0 < \mbox{\code{grad\_samples}}$}
      {Defaults to \code{1}}
%
    \hiercmdarg{\indentarrow\indentarrow}{elbo\_samples}{$$int$$}
      {Number of samples for Monte Carlo estimate of ELBO (objective function)}
      {Valid values: \ $0 < \mbox{\code{elbo\_samples}}$}
      {Defaults to \code{100}}
%
    \hiercmdarg{\indentarrow\indentarrow}{eta\_adagrad}{$$double$$}
      {Stepsize weighting parameter for adaptive stepsize sequence}
      {Valid values: \ $0 < \mbox{\code{eta\_adagrad}} \leq 1.0$}
      {Defaults to \code{0.1}}
%
    \hiercmdarg{\indentarrow\indentarrow}{iter}{$$int$$}
      {Maximum number of iterations}
      {Valid values: \ $0 < \mbox{\code{iter}}$}
      {Defaults to \code{10000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{tol\_rel\_obj}{$$double$$}
      {Convergence tolerance on the relative norm of the objective}
      {Valid values: \ $0 < \mbox{\code{tol\_rel\_obj}}$}
      {Defaults to \code{0.01}}
%
    \hiercmdarg{\indentarrow\indentarrow}{eval\_elbo}{$$int$$}
      {Evaluate ELBO every Nth iteration}
      {Valid values: \ $0 < \mbox{\code{eval\_elbo}}$}
      {Defaults to \code{100}}
%
    \hiercmdarg{\indentarrow\indentarrow}{output\_samples}{$$int$$}
      {Number of posterior samples to draw and save}
      {Valid values: \ $0 < \mbox{\code{output\_samples}}$}
      {Defaults to \code{1000}}
\end{description}
%
% The following options are for the (L-)BFGS optimizer.  BFGS is the default
% optimizer and also much faster than the other optimizers.

% Convergence monitoring in (L-)BFGS is controlled by a number of tolerance
% values, any one of which being satisified causes the algorithm to
% terminate with a solution.

\subsection{Diagnostic-Specific Arguments}

The following arguments are specific to diagnostics.  As of now, the
only diagnostic is gradients of the log probability function.

\begin{description}

  \hierlongcmd{\indentarrow}{\farg{\bfseries diagnose}}
    {Model diagnostics}
    {Valid subarguments: \code{test}}
%
    \hiercmdarg{\indentarrow\indentarrow}{test}{$$list element$$}
      {Diagnostic test}
      {Valid values: \  \code{gradient}}
      {Defaults to \code{gradient}}
%
      \hiershortcmd{\indentarrow\indentarrow\indentarrow}{\farg{gradient}}
        {Check model gradient against finite differences}
        {Valid subarguments: \code{epsilon}, \code{error}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{epsilon}{$$real$$}
        {Finite difference step size}
	    {Valid values:\ $0 < \mbox{\code{epsilon}}$}
	    {Defaults to \code{1e-6}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{error}{$$real$$}
       {Error threshold}
	   {Valid values:\ $0 < \mbox{\code{error}}$}
	   {Defaults to \code{1e-6}}
%
\end{description}

\subsection{General-Purpose Arguments}

The following arguments may be used with any of the previous
configurations.   They may come either before or after the other
subarguments of the top-level method.

\subsubsection{Process Identifier Argument}

\begin{description}
\hiercmdarg{}{id}{$$int$$}
  {Unique process identifier, used to advance random number generator so that random numbers do not overlap across chains}
  {Valid values: \  $0 < \mbox{\code{id}}$}
  {Defaults to \code{0}}
%
\end{description}

\subsubsection{Input Data Arguments}

\begin{description}

\hierlongcmd{}{data}
  {Input data options}
  {Valid subarguments: \code{file}}
%
  \hiercmdarg{\indentarrow}{file}{$$string$$}
    {Input data file}
    {Valid values: \  Path to existing file}
    {Defaults to empty path}
%
\end{description}

\subsubsection{Initialization Arguments}

Initialization is only applied to parameters defined in the parameters
block.  Any initial values supplied for transformed parameters or
generated quantities are ignored.

\begin{description}
\hiercmdarg{}{init}{$$string$$}
  {Initialization method: \\
        \hspace*{8pt} $\bullet$ \ real number $\mbox{\farg{x}} > 0$ initializes randomly bewteen [-\farg{x},
        \farg{x}];
        \\
        \hspace*{8pt} $\bullet$ \  \code{0} initializes to 0;
        \\
        \hspace*{8pt} $\bullet$ \  non-number interpreted as a data file}
  {Valid values: \  All}
  {Defaults to \code{2}}
%
\end{description}


\subsubsection{Random Number Generator Arguments}

\begin{description}

\hierlongcmd{}{{\bfseries random}}
  {Random number configuration}
  {Valid subarguments: \code{seed}}
%
  \hiercmdarg{\indentarrow}{seed}{$$unsigned int$$}
    {Random number generator seed}
    {Valid values: \\
      \hspace*{8pt} $\bullet$ \ $\mbox{\code{seed}} \geq 0$ generates seed;
      \\
      \hspace*{8pt} $\bullet$ \ $\mbox{\code{seed}} < 0$ uses seed generated from time}
    {Defaults to \code{-1}}
%
\end{description}

\subsubsection{Output Arguments}

\begin{description}
\hierlongcmd{}{{\bfseries output}}
  {File output options}
  {Valid subarguments: \code{file, diagnostic\_file, refresh}}
%
  \hiercmdarg{\indentarrow}{file}{$$string$$}
    {Output file}
    {Valid values: \  Valid path}
    {Defaults to \code{output.csv}}
%
  \hiercmdarg{\indentarrow}{diagnostic\_file}{$$string$$}
    {Auxiliary output file for diagnostic information}
    {Valid values: \  Valid path}
    {Defaults to empty path}
%
  \hiercmdarg{\indentarrow}{refresh}{$$int$$}
    {Number of interations between screen updates}
    {Valid values: \  $0 < \mbox{\code{refresh}}$}
    {Defaults to \code{100}}
%
\end{description}

\section{Command-Line Option Examples}

The hierarchical structure of the command-line options can be
intimidating, and here we provide an example workflow to help ease the
introduction to new users, especially those used to \Stan 1.3 or
earlier releases.  The examples in this section are for Mac OS and
Linux; on Windows, just remove the \code{./} before the executable and
change the line-continuation character from Unix's
\code{\textbackslash} to DOS's \code{\textasciicircum}.  As in
previous sections, the indentation on continued lines is for
pedagogical purposes only and does not convey any content to the
executable.

Let's say that we've just built our model, \code{model}, and are ready to run.
We begin by specifying data and init files,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model data file=model.data.R init=model.init.R
\end{Verbatim}
\end{quote}
%
but our model doesn't run.  Instead, the above command prints
%
\begin{quote}
\begin{Verbatim}
A method must be specified!
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
The problem is that we forgot to specify a method.

All \Stan arguments have default values, except for the method.  This
is the only argument that must be specified by the user and a model
will not run without it (not to say that the model will run without error,
for example a model that requires data will eventually fail unless an input file
is specified with \code{file} under \code{data}).  Assuming that we want to draw
MCMC samples from our model, we can either specify a method
implicitly,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model sample data file=model.data.R init=model.init.R
\end{Verbatim}
\end{quote}
%
or explicitly,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data file=model.data.R  \
          init=model.init.R
\end{Verbatim}
\end{quote}
%
In either case our model now executes without any problem.

Now let's say that we want to customize our execution.  In
particular we want to set the seed for the random number generator,
but we forgot the specific argument syntax.  Information for each
argument can displayed by calling \code{help},
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model random help
\end{Verbatim}
\end{quote}
%
which returns
%
\begin{quote}
\begin{Verbatim}
random
  Random number configuration
  Valid subarguments: seed
...
\end{Verbatim}
\end{quote}
%
before printing usage information.  For information on the
seed argument we just call help one level deeper,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model random seed help
\end{Verbatim}
\end{quote}
%
which returns
%
\begin{quote}
\begin{Verbatim}
seed=<unsigned int>
  Random number generator seed
  Valid values: seed > 0, if negative seed is generated from time
  Defaults to -1
  ...
\end{Verbatim}
\end{quote}
%
Fully informed, we can now run with a given seed,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data fle=model.data.R  \
          init=model.init.R                    \
          random seed=5
\end{Verbatim}
\end{quote}

The arguments \code{method}, \code{data}, \code{init}, and
\code{random} are all top-level arguments.  To really see the power of
a hierarchical argument structure let's try to drill down and specify
the metric we use for HMC: instead of the default diagonal Euclidean
metric, we want to use a dense Euclidean metric.  Attempting to
specify the metric we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data file=model.data.R  \
          init=model.init.R                     \
          random seed=5                         \
          metric=unit
\end{Verbatim}
\end{quote}
%
only to have the execution fail with the message
%
\begin{quote}
\begin{Verbatim}
metric=unit_e is either mistyped or misplaced.
Perhaps you meant one of the following valid configurations?
  method=sample algorithm=hmc metric=<list_element>
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
The argument \code{metric} does exist, but not at the top-level.  In order
to specify it we have to drill down into sample by first specifying the
sampling algorithm, as noted in the suggestion,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample algorithm=hmc metric=unit \
          data file=model.data.R                  \
          init=model.init.R                       \
          random seed=5
\end{Verbatim}
\end{quote}
%
Unfortunately we still messed up,
%
\begin{quote}
\begin{Verbatim}
unit is not a valid value for "metric"
  Valid values: unit_e, diag_e, dense_e
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
Tweaking the metric name we make one last attempt,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample algorithm=hmc metric=unit_e \
          data file=model.data.R                    \
          init=model.init.R                         \
          random seed=5
\end{Verbatim}
\end{quote}
%
which successfully runs.

Finally, let's consider the circumstance where our model runs fine but
the NUTS iterations keep saturating the default tree depth limit of 10.  We need
to change the limit, but how do we specify NUTS let alone the maximum tree depth?
To see how let's take advantage of the \code{help-all} option which prints all
arguments that derive from the given argument.  We know that NUTS is somehow
related to sampling, so we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample help-all
\end{Verbatim}
\end{quote}
%
which returns the verbose output,
%
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
sample
  Bayesian inference with Markov Chain Monte Carlo
  Valid subarguments: num_samples, num_warmup,
                      save_warmup, thin, adapt, algorithm

  num_samples=<int>
    Number of sampling iterations
    Valid values: 0 <= num_samples
    Defaults to 1000

  num_warmup=<int>
    Number of warmup iterations
    Valid values: 0 <= warmup
    Defaults to 1000

  save_warmup=<boolean>
    Stream warmup samples to output?
    Valid values: [0, 1]
    Defaults to 0

  thin=<int>
    Period between saved samples
    Valid values: 0 < thin
    Defaults to 1

  adapt
    Warmup Adaptation
    Valid subarguments: engaged, gamma, delta, kappa, t0

    engaged=<boolean>
      Adaptation engaged?
      Valid values: [0, 1]
      Defaults to 1

    gamma=<double>
      Adaptation regularization scale
      Valid values: 0 < gamma
      Defaults to 0.05

    delta=<double>
      Adaptation target acceptance statistic
      Valid values: 0 < delta < 1
      Defaults to 0.65

    kappa=<double>
      Adaptation relaxation exponent
      Valid values: 0 < kappa
      Defaults to 0.75

    t0=<double>
      Adaptation iteration offset
      Valid values: 0 < t0
      Defaults to 10

  algorithm=<list element>
    Sampling algorithm
    Valid values: hmc
    Defaults to hmc

    hmc
      Hamiltonian Monte Carlo
      Valid subarguments: engine, metric, stepsize,
                          stepsize_jitter

      engine=<list element>
        Engine for Hamiltonian Monte Carlo
        Valid values: static, nuts
        Defaults to nuts

        static
          Static integration time
          Valid subarguments: int_time

          int_time=<double>
            Total integration time for Hamiltonian evolution
            Valid values: 0 < int_time
            Defaults to 2 * pi

        nuts
          The No-U-Turn Sampler
          Valid subarguments: max_depth

          max_depth=<int>
            Maximum tree depth
            Valid values: 0 < max_depth
            Defaults to 10

      metric=<list element>
        Geometry of base manifold
        Valid values: unit_e, diag_e, dense_e
        Defaults to diag_e

        unit_e
          Euclidean manifold with unit metric

        diag_e
          Euclidean manifold with diag metric

        dense_e
          Euclidean manifold with dense metric

      stepsize=<double>
        Step size for discrete evolution
        Valid values: 0 < stepsize
        Defaults to 1

      stepsize_jitter=<double>
        Uniformly random jitter of the stepsize, in percent
        Valid values: 0 <= stepsize_jitter <= 1
        Defaults to 0
...
\end{Verbatim}
\end{quote}
%
Following the hierarchy, the maximum tree depth derives from \code{nuts},
which itself is a value for the argument \code{engine} which derives from
\code{hmc}.  Adding this to our previous call we attempt
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample                       \
              algorithm=hmc                   \
                  metric=unit_e               \
                  engine=nuts max_depth=-15   \
          data file=model.data.R              \
          init=model.init.R                   \
          random seed=5                       \
\end{Verbatim}
\end{quote}
%
which yields
%
\begin{quote}
\begin{Verbatim}
-1 is not a valid value for "max_depth"
  Valid values: 0 < max_depth
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
Where did that negative sign come from?  Clumsy fingers are nothing
to be embarrassed about, especially with such complex argument
configurations.  Removing the guilty character, we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample                     \
              algorithm=hmc                 \
                  metric=unit_e             \
                  engine=nuts max_depth=15  \
          data file=model.data.R            \
          init=model.init.R                 \
          random seed=5
\end{Verbatim}
\end{quote}
%
which finally runs without issue.

\section{Command Templates}

This section provides templates for all of the arguments deriving from
each of the possible methods: \code{sample}, \code{optimize}, \code{variational}
and \code {diagnose}.
Arguments in square brackets are optional,
those not in square brackets are required for the template.

\subsection{Sampling Templates}
%
The No-U-Turn sampler (NUTS) is the default (and recommended) sampler
for Stan.  The full set of configuration options is in
\reffigure{nuts-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model sample                                     \
                 algorithm=hmc                          \
                     engine=nuts                        \
                       [max_depth=<int>]                \
                     [metric={unit_e,diag_e,dense_e}]   \
                     [stepsize=<double>]                \
                     [stepsize_jitter=<double>]         \
                 [num_samples=<int>]                    \
                 [num_warmup=<int>]                     \
                 [save_warmup=<boolean>]                \
                 [thin=<int>]                           \
                 [adapt                                 \
                      [engaged=<boolean>]               \
                      [gamma=<double>]                  \
                      [delta=<double>]                  \
                      [kappa=<double>]                  \
                      [t0=<double>] ]                   \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the no-U-turn sampler
  (NUTS). This is the same skeleton as that for basic HMC in
  \reffigure{hmc-command}.  Elements in braces are optional.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{nuts-command.figure}
\end{figure}
%

A standard Hamiltonian Monte Carlo (HMC) sampler with user-specified
integration time may also be used.  Its set of configuration options
are shown in \reffigure{hmc-command}.

Both NUTS and HMC may be configured with either a unit,
diagonal or dense Euclidean metric, with a diagonal metric the default.%
%
\footnote{In Euclidean HMC, a diagonal metric emulates different step
sizes for each parameter.  Explicitly varying step sizes were used in
Stan 1.3 and before; \cite{Neal:2011} discusses the equivalence.}
%
A unit metric provides no parameter-by-parameter scaling,
a diagonal metric scales each parameter independently, and
a dense metric also rotates the parameters so
that correlated parameters may move together.  Although dense metrics
offer the hope of superior simulation performance, they
require more computation per iteration.  Specifically for $m$ samples of a model with
$n$ parameters, the dense metric requires $\mathcal{O}(n^3 \log(m) +
n^2 \, m)$ operations, whereas diagonal metrics require only
$\mathcal{O}(n \, m)$.  Furthermore, dense metrics are difficult
to estimate, given the $\mathcal{O}(n^2)$ components with complex
interdependence.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model sample                                     \
                 algorithm=hmc                          \
                     engine=static                      \
                       [int_time=<double>]              \
                     [metric={unit_e,diag_e,dense_e}]   \
                     [stepsize=<double>]                \
                     [stepsize_jitter=<double>]         \
                 [num_samples=<int>]                    \
                 [num_warmup=<int>]                     \
                 [save_warmup=<boolean>]                \
                 [thin=<int>]                           \
                 [adapt                                 \
                      [engaged=<boolean>]               \
                      [gamma=<double>]                  \
                      [delta=<double>]                  \
                      [kappa=<double>]                  \
                      [t0=<double>] ]                   \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the basic Hamiltonian
  Monte Carlo sampler (HMC).  This is the same as the NUTS command
  skeleton shown in \reffigure{nuts-command} other than for the
  engine.  Elements in braces are optional.  All arguments and their
  default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{hmc-command.figure}
\end{figure}

\subsection{Optimization Templates}
%
\CmdStan supports several optimizers.  These share many of their
configuration options with the samplers.  The default optimizer is the
the limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method;
\cite{NocedalWright:2006} contains an excellent overview of both the
BFGS and L-BFGS algorithms.  The command skeleton for L-BFGS is in
\reffigure{l-bfgs-command} and the one for BFGS is in
\reffigure{bfgs-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=lbfgs                         \
                     [init_alpha=<double>]              \
                     [tol_obj=<double>]                 \
                     [tol_rel_obj=<double>]             \
                     [tol_grad=<double>]                \
                     [tol_rel_grad=<double>]            \
                     [tol_param=<double>]               \
                     [history_size=<int>]               \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the L-BFGS optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{l-bfgs-command.figure}
\end{figure}
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=bfgs                         \
                     [init_alpha=<double>]              \
                     [tol_obj=<double>]                 \
                     [tol_rel_obj=<double>]             \
                     [tol_grad=<double>]                \
                     [tol_rel_grad=<double>]            \

                     [tol_param=<double>]               \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the BFGS optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{bfgs-command.figure}
\end{figure}
%
Stan also supports Newton's method; see \citep{NocedalWright:2006} for
more information.  This method is the least efficient of the three,
but has the advantage of setting its own step size.  Other than not
having a stepsize argument, the skeleton for Newton's method shown in
\reffigure{newton-command} is identical to that for BFGS.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=newton                       \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the Newton optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{newton-command.figure}
\end{figure}
%

\subsection{Variational Inference Templates}
%
\CmdStan implements Automatic Differentiation Variational Inference
\citep{Kucukelbir:2015}. The command skeleton for the \code{meanfield}
algorithm is in \reffigure{meanfield-command}. The command skeleton for the
\code{fullrank} algorithm is in \reffigure{fullrank-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model variational                                \
                 algorithm=meanfield                    \
                     [grad_samples=<int>]               \
                     [elbo_samples=<int>]               \
                     [eta_adagrad=<double>]             \
                     [iter=<int>]                       \
                     [tol_rel_obj=<double>]             \
                     [eval_elbo=<int>]                  \
                     [output_samples=<int>]             \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the meanfield variational
  inference algorithm.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{meanfield-command.figure}
\end{figure}
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model variational                                \
                 algorithm=fullrank                     \
                     [grad_samples=<int>]               \
                     [elbo_samples=<int>]               \
                     [eta_adagrad=<double>]             \
                     [iter=<int>]                       \
                     [tol_rel_obj=<double>]             \
                     [eval_elbo=<int>]                  \
                     [output_samples=<int>]             \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the fullrank variational
  inference algorithm.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{fullrank-command.figure}
\end{figure}


\subsection{Diagnostic Command Skeleton}

Stan reports on gradients for the model at a specified or randomly
generated initial value.  The command-skeleton in this case is very
simple, and shown in \reffigure{diagnostic-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model diagnose                  \
                 [test=gradient]       \
                     [epsilon=<real>]  \
                     [error=<real>]    \
             [data file=<string>]      \
             [init=<string>]           \
             [random seed=<int>]       \
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking model diagnostics.  All
  arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{diagnostic-command.figure}
\end{figure}

