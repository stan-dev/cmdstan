\chapter{{\tt\bfseries diagnose}: Diagnosing Biased Hamiltonian Monte Carlo Inferences}\label{diagnose.chapter}

\noindent
\CmdStan is distributed with a utility that is able to read in and
analyze the output of one or more Markov chains to check for the
following potential problems:

\begin{itemize}
\item Divergent transitions
\item Transitions that hit the maximum treedepth
\item Low E-BFMI values
\item Low effective sample sizes
\item High $\hat{R}$ values
\end{itemize}

The meanings of several of these problems are discussed in \url{https://arxiv.org/abs/1701.02434}.


\section{Building the {\tt\bfseries diagnose} Command}

\CmdStan's \code{diagnose} command is built along with \code{stanc} into
the \code{bin} directory. It can be compiled directly using the
makefile as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <cmdstan-home>
> make bin/diagnose
\end{Verbatim}
\end{quote}
%

\section{Running the {\tt\bfseries diagnose} Command}

The \code{diagnose} command is executed on one or more output files,
which are provided as command-line arguments separated by spaces.
If there are no apparent problems with the output files passed to
\code{bin/diagnose}, it outputs a message that all transitions
are within treedepth limit and that no divergent transitions were found.

It problems are detected, it outputs a summary of the problem along with
possible ways to mitigate it. As an example,  we use the
the ``eight schools'' model from Stan's example models and
its corresponding data.%
%
\footnote{The model and associated data files are here:
  \begin{itemize}
  \item
    \url{https://github.com/stan-dev/example-models/blob/master/misc/eight_schools/eight_schools.stan}
  \item
    \url{https://github.com/stan-dev/example-models/blob/master/misc/eight_schools/eight_schools.data.R}
  \end{itemize}
}
The model is run with four chains and the random seed 12345, leaving
the output files \code{eight\_schools1.csv}, \code{eight\_schools2.csv},
etc. The \code{diagnose} command is then run as follows:
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bin/diagnose eight_schools*.csv
\end{Verbatim}
\end{quote}
The result of \code{bin/diagnose} is displayed in
\reffigure{bin-diagnose-eg}, indicating two problems, one with
divergent transitions, and one indicating a low E-BFMI, and possible
ways to solve these problems. The first problem indicates that the
parameter \code{delta} of the sampling algorithm needs to be
increased. Since the contents of \code{eight\_schools1.csv} contains
the lines
\begin{quote}
\begin{Verbatim}
#     adapt
#       engaged = 1 (Default)
#       gamma = 0.050000000000000003 (Default)
#       delta = 0.80000000000000004 (Default)
\end{Verbatim}
\end{quote}
this suggests that \code{delta} should be increased beyond
0.8. Following section~\ref{detailed-command-arguments.section}, this
suggests that the model perhaps should be rerun as follows:
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> for i in {1..4}
  do
    ./eight_schools sample adapt delta=0.9 \
       random seed=12345 id=$i data \
       file=eight_schools.data.R \
       output file=eight_schools$i.csv &
  done
\end{Verbatim}
\end{quote}


\begin{figure}
\begin{Verbatim}[fontsize=\footnotesize]
 Checking sampler transitions for divergences.
 95 of 4000 (2.4%) transitions ended with a divergence.
 These divergent transitions indicate that HMC is not fully
 able to explore the posterior distribution.
 Try increasing adapt delta closer to 1.
 If this doesn't remove all divergences, try to reparameterize the model.

 Checking E-BFMI - sampler transitions HMC potential energy.
 The E-BFMI, 0.27, is below the nominal threshold of 0.3 which suggests
 that HMC may have trouble exploring the target distribution.
 If possible, try to reparameterize the model.
\end{Verbatim}
\caption{Example output from \code{bin/diagnose}.}
\label{bin-diagnose-eg.figure}
\end{figure}


\section{Warnings and Recommendations}

\subsection{Divergent transitions after warmup}

Stan uses Hamiltonian Monte Carlo (HMC) to explore the target distribution ---
the posterior defined by a Stan program + data --- by simulating the evolution
of a \href{https://en.wikipedia.org/wiki/Hamiltonian_system}{Hamiltonian system}.
In order to approximate the exact solution of the Hamiltonian dynamics we need to
choose a step size governing how far we move each time we evolve the system
forward. That is, the {\it step size controls the resolution of the sampler}

Unfortunately, for particularly hard problems there are features of the target 
distribution that are too small for this resolution. Consequently the sampler 
misses those features and returns biased estimates. Fortunately, this mismatch 
of scales manifests as {\it divergences} which provide a practical diagnostic.

If the divergent transitions cannot be eliminated by increasing the \code{adapt\_delta}
parameter, we have to find a different way to write the model that is logically
equivalent but simplifies the geometry of the posterior distribution. This
problem occurs frequently with hierarchical models and one of the simplest
examples is Neal's Funnel, which is discussed in the
\href{https://mc-stan.org/docs/2_22/stan-users-guide/reparameterization-section.html}
{reparameterization section} of the Stan User's Guide.

\subsection{Maximum treedepth exceeded}

Warnings about hitting the maximum treedepth are not as serious as warnings 
about divergent transitions. While divergent transitions are a {\it validity}
concern, hitting the maximum treedepth is an {\it efficiency} concern. Configuring
the No-U-Turn-Sampler (the variant of HMC used by Stan) involves putting a cap
on the depth of the trees that it evaluates during each iteration (for details
on this see the *Hamiltonian Monte Carlo Sampling* chapter in the 
\href{https://mc-stan.org/docs/reference-manual/hmc-chapter.html}{Stan Reference Manual}).
When the maximum allowed tree depth is 
reached it indicates that NUTS is terminating prematurely to avoid excessively 
long execution time.

This is controlled through the \code{max\_depth} argument.
If the number of transitions which exceed maximum treedepth is low, increasing
\code{max\_depth} may correct this problem.


\subsection{Low E-BFMI values}

You may see a warning that says some number of chains had an estimated Bayesian
Fraction of Missing Information (BFMI) that was too low. This implies that the
adaptation phase of the Markov Chains did not turn out well and those chains
likely did not explore the posterior distribution efficiently. For more details
on this diagnostic, see \url{https://arxiv.org/abs/1604.00695}.
Should this occur, you can either run the sampler for more iterations, or consider
reparameterizing your model.

\subsection{Low effective sample sizes}


Roughly speaking, the effective sample size (ESS) of a quantity of
interest captures how many independent draws contain the same amount
of information as the dependent sample obtained by the MCMC
algorithm. Clearly, the higher the ESS the better.  Stan uses $\hat{R}$
adjustment to use the between-chain information in computing the
ESS. For example, in case of multimodal distributions with
well-separated modes, this leads to an ESS estimate that is close to
the number of distinct modes that are found.

Bulk-ESS refers to the effective sample size based on the rank
normalized draws. This does not directly compute the ESS relevant for
computing the mean of the parameter, but instead computes a quantity
that is well defined even if the chains do not have finite mean or
variance. Overall bulk-ESS estimates the sampling efficiency for the
location of the distribution (e.g. mean and median).

Often quite smaller ESS would be sufficient for the desired estimation
accuracy, but the estimation of ESS and convergence diagnostics
themselves require higher ESS.  We recommend requiring that the
bulk-ESS is greater than 100 times the number of chains. For example,
when running four chains, this corresponds to having a rank-normalized
effective sample size of at least 400.


\subsection{High $\hat{R}$ values}

$\hat{R}$ (R-hat) convergence diagnostic compares the between- and within-chain
estimates for model parameters and other univariate quantities of
interest. If chains have not mixed well (ie, the between- and
within-chain estimates don't agree), $\hat{R}$ is larger than 1. We
recommend running at least four chains by default and only using the
sample if $\hat{R}$ is less than 1.01. Stan reports $\hat{R}$ which is the
maximum of rank normalized split-R-hat and rank normalized
folded-split-R-hat, which works for thick tailed distributions and is
sensitive also to differences in scale. For more details on this
diagnostic, see https://arxiv.org/abs/1903.08008.

There is further discussion in  \url{https://arxiv.org/abs/1701.02434});
however the correct resolution is necessarily model specific,
hence all suggestions general guidelines only.

