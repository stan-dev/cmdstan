\chapter{Getting Started}

\noindent
This chapter is designed to help users get acquainted with the
\CmdStan interface. Later chapters are devoted to expanding on the
material in this chapter with full reference documentation. See the
Stan user's manual for details about the Stan language.

\section{Installation}

Installation of \CmdStan is simple. \CmdStan requires:
\begin{itemize}
\item The \CmdStan source code and all its libraries. \\
  This is included in the release tarball or zip file as a single
  download.
\item The \code{make} utility program. \\
  This is not strictly necessary, but will make the build process
  easy. On Windows the \code{mingw32-make} variant is needed to build
  \CmdStan. This is a \code{make} variant and can be installed as part
  of RTools (\url{https://cran.rstudio.com/bin/windows/Rtools/}). The
  rest of the documentation assumes \code{make} (\code{mingw32-make}
  on Windows) is available.
\item A \Cpp compiler.
\end{itemize}

\noindent For information about supported versions of Windows, Mac, and Linux
platforms see \refappendix{install}. For step-by-step installation
instructions of the prerequisites, see
\begin{itemize}
  \item Windows: \refappendix{install-windows}
  \item Mac: \refappendix{install-mac}
  \item Linux: \refappendix{install-linux}.
\end{itemize}


\section{Building \CmdStan}

Building \CmdStan involves preparing a set of executable programs:
%
\begin{itemize}
\item \code{stanc}: the Stan compiler (translates Stan language to \Cpp).
\item \code{stansummary}: a basic posterior analysis tool.
\item \code{diagnose}: a basic sampler diagnostic tool.
\end{itemize}
%
\CmdStan releases include pre-built binaries of the Stan language compiler
\url {https://github.com/stan-dev/stanc3}: \code{bin/linux-stanc}, \code{bin/mac-stanc} and \code{bin/windows-stanc}.
The CmdStan makefile \code{build} task copies the appropriate binary to \code{bin/stanc}.  For CmdStan installations
which have been cloned of downloaded from the CmdStan GitHub repository, 
the makefile task will download the appropriate OS-specific binary
from the stanc3 repository's nightly release.
%
The build process utilizes the \code{GNU make} command-line
utility or the Windows \code{mingw32-make} equivalent.

Steps to build \CmdStan:
%
\begin{enumerate}
  \item Open a command-line terminal window and change directories to
    the \CmdStan directory. From here on, we'll refer to this location as
    \code{<cmdstan-home>}.
    %
    \begin{quote}
      \begin{Verbatim}[fontshape=sl]
> cd <cmdstan-home>
      \end{Verbatim}
    \end{quote}
    %
  \item \emph{Optional:} \, Set local \code{make} variables by editing
    the file \verb+~/.config/stan/make.local+ or
    \verb+<cmdstan-home>/make/local+. See \refappendix{make-options}
    for a list of available options.  For most installations, this
    step can be skipped; the default configuration should work for
    most users.
    %
  \item Use make to build \CmdStan. When multiple CPU cores are
    available on the system, the call to make can be parallelized.  It
    can either be specified directly when calling \code{make} with the
    \code{-j{\slshape N}} option, where \code{{\slshape N}} is the
    number of CPU cores. For instance, to run on 4 cores, use
    %
    \begin{quote}
      \begin{Verbatim}[fontshape=sl]
> make build -j4
      \end{Verbatim}
    \end{quote}
    %
    \textbf{Warning:} \ The \code{make} program may take 10+ minutes and
    consume 2+ GB of memory to build \CmdStan. Please use
    \code{mingw32-make} on Windows (available from RTools).
  \item \emph{Windows only:} \, \CmdStan requires that the Intel TBB
    library, which is build by the above command, can be found by the
    Windows system. This requires that the directory
    \verb+<cmdstan-home>/stan/lib/stan_math/lib/tbb+ is part of the
    \code{PATH} environment variable. To permanently make this setting
    for the current user, you may execute:
    %
    \begin{quote}
      \begin{Verbatim}[fontshape=sl]
> mingw32-make install-tbb
      \end{Verbatim}
    \end{quote}
    %
    Don't forget to open a new shell where these new settings will
    take effect. For other platforms this step is not needed, since
    the absolute path to the Intel TBB library is linked into Stan
    programs (not possible on Windows).
\end{enumerate}
%
When \CmdStan is successfully built, the \code{make} program will
report (after other lines of output)
%
\begin{quote}
  \begin{Verbatim}
--- CmdStan v2.21.0 built ---
  \end{Verbatim}
\end{quote}
%
and there will be the following executables in the \code{<cmdstan-home>/bin/} folder:
%
\begin{itemize}
  \item \code{stanc}, the Stan compiler. The Stan compiler
    translates a Stan program into \Cpp code. See \refchapter{stanc}
    for details.
  \item \code{stansummary}, a posterior analysis tool. The \code{stansummary}
    command summarizes the comma-separated values files that are
    generated from Stan program runs. For each parameter within the
    Stan program, \code{stansummary} reports the mean, standard deviation,
    quantiles, $\hat{R}$, and other values. See \refchapter{stansummary} for
    details.
  \item \code{diagnose}, a sampler analysis tool. The \code{diagnose}
    reads the generated outputs from Stan program runs and checks for
    indications that the HMC sampler was unable to sample from the full posterior.

\end{itemize}

\section{Compiling and Executing a Stan Program}\label{compiling-model.section}

The rest of this quick-start guide explains how to code and run a very
simple Bayesian model.

\subsection{A Simple Bernoulli Model}

The following is a simple, complete Stan program for a Bernoulli model
of binary data.%
%
\footnote{The model is available with the CmdStan distribution at the path
  \nolinkurl{examples/bernoulli/bernoulli.stan}.  }
%
\begin{quote}
\begin{Verbatim}
data {
  int<lower=0> N;
  int<lower=0,upper=1> y[N];
}
parameters {
  real<lower=0,upper=1> theta;
}
model {
  theta ~ beta(1,1);  // uniform prior on interval 0,1
  y ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
The model assumes the binary observed data \code{y[1],...,y[N]}
are i.i.d.\ with Bernoulli chance-of-success \code{theta}.  The
prior on \code{theta} is \code{beta(1,1)} (i.e., uniform).

\subsection{Data Set}

A data set of $\mbox{\code{N}}=10$ observations is coded either
using JSON notation or in the dump data format created by
the \code{stan\_rdump} in package \code{rstan}.
%
\footnote{
The data is also included with the CmdStan distribution and can be found at path
\nolinkurl{examples/bernoulli/bernoulli.data.R} and \nolinkurl{examples/bernoulli/bernoulli.data.json} .
}
In JSON:
%
\begin{quote}
\begin{Verbatim}
{ "N" : 10, "y" : [ 0, 1, 0, 0, 0, 0, 0, 0, 0, 1 ] }
\end{Verbatim}
\end{quote}
%
In \code{stan\_rdump} format:
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
This defines the contents of two variables, \code{N} and \code{y},
using an R-like syntax (see \refchapter{dump} for more information).

\subsection{Change directories to \code{<cmdstan-home>}}

Before building any Stan program, change directories to \code{<cmdstan-home>}.

\subsection{Compiling a Stan Program}

A single call to \code{make} is all that's necessary to translate a
Stan program to an executable for the command line. (This call will
first translate the Stan program to \Cpp, then compile the \Cpp code
to an executable.)

A Stan program must be in a file with the file extension
\code{.stan}. To create an executable from a Stan program, \code{make}
will be called with the name of the executable as its argument. For
Mac and Linux, it is the name of the Stan program with the
\code{.stan} omitted. For Windows, replace \code{.stan} with \code{.exe},
and make sure that the path is given with slashes and not backslashes.

To build the Bernoulli example, use the following command for Mac and
Linux:
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make examples/bernoulli/bernoulli
\end{Verbatim}
\end{quote}
%
For Windows, the command is the same with the addition of \code{.exe}
at the end of the target (note: use forward slashes):
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make examples/bernoulli/bernoulli.exe
\end{Verbatim}
\end{quote}

The generated \Cpp code (\code{bernoulli.hpp}) and the compiled
executable will be placed in the same directory as the Stan program.

\textbf{Note:} you must start in the \code{<cmdstan-home>}
directory. The Stan program can be in a different path, but the path
to the Stan program must not contain a space. (This is a limitation
that's introduced by \code{make}.) Relative paths are ok; the relative
path must not contain a space.

\subsection{Sampling from the Stan Program}

The program can be executed from the directory in which it resides.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd examples/bernoulli
\end{Verbatim}
\end{quote}
%
To execute sampling of the model under Linux or Mac, use
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
In Windows, the \code{./} prefix is not needed, resulting in the
following command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli.exe sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
The output is the same across all supported platforms.  First, the
configuration of the program is echoed to the standard output:
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
 method = sample (Default)
   sample
     num_samples = 1000 (Default)
     num_warmup = 1000 (Default)
     save_warmup = 0 (Default)
     thin = 1 (Default)
     adapt
       engaged = 1 (Default)
       gamma = 0.050000000000000003 (Default)
       delta = 0.80000000000000004 (Default)
       kappa = 0.75 (Default)
       t0 = 10 (Default)
       init_buffer = 75 (Default)
       term_buffer = 50 (Default)
       window = 25 (Default)
     algorithm = hmc (Default)
       hmc
         engine = nuts (Default)
           nuts
             max_depth = 10 (Default)
         metric = diag_e (Default)
         metric_file =  (Default)
         stepsize = 1 (Default)
         stepsize_jitter = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)
\end{Verbatim}
\end{quote}
%
After the configuration has been displayed, a short timing message is
given.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
Gradient evaluation took 4e-06 seconds
1000 transitions using 10 leapfrog steps per transition would
  take 0.04 seconds.
Adjust your expectations accordingly!
\end{Verbatim}
\end{quote}
%
Next, the sampler reports the iteration number, reporting the
percentage complete.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:  100 / 2000 [  5%]  (Warmup)
...
Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}

\subsubsection{Sampler Output}

Each execution of the model results in draws from a single Markov
chain being written to a file in comma-separated value (CSV) format.
The default name of the output file is \nolinkurl{output.csv}.

The first part of the output file records the version of the
underlying Stan library and the configuration as comments (i.e.,
lines beginning with the pound sign (\Verb|#|)).
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 12
# stan_version_patch = 0
# model = bernoulli_model
# method = sample (Default)
#   sample
#     num_samples = 1000 (Default)
#     num_warmup = 1000 (Default)
#     save_warmup = 0 (Default)
#     thin = 1 (Default)
...
\end{Verbatim}
\end{quote}
%
This is followed by a CSV header indicating the names of the values
sampled.
%
\begin{Verbatim}[fontsize=\small]
lp__,accept_stat__,stepsize__,treedepth__,n_leapfrog__,divergent__,energy__,theta
\end{Verbatim}
%
The first column reports the unnormalized log probability of the
model.  The next columns provide sampler-dependent information; here,
it is columns two through five. For basic Hamiltonian Monte Carlo
(HMC) and its adaptive variant, the No-U-Turn sampler (NUTS), the
sampler-dependent parameters are described in the following table.
%
\begin{center}
\begin{tabular}{l|l|l}
{\it Sampler} & {\it Parameter} & {\it Description}
\\ \hline \hline
\NUTS & \Verb| accept_stat__  | & Metropolis acceptance probability
\\
& & averaged over samples in the slice
\\
\NUTS & \Verb| stepsize__ | & Integrator step size
\\
\NUTS & \Verb| treedepth__  | & Tree depth
\\
\NUTS & \Verb| n_leapfrog__  | & Number of leapfrog calculations
\\
\NUTS & \Verb| divergent__  | & 1 if trajectory diverged
\\
\NUTS & \Verb| energy__ | & Hamiltonian value
\\ \hline
\HMC & \Verb| accept_stat__ | &  Metropolis acceptance probability
\\
\HMC & \Verb| stepsize__  | & Integrator step size
\\
\HMC & \Verb| int_time__  | & Total integration time
\\
\NUTS & \Verb| energy__ | & Hamiltonian value
\\
\end{tabular}
\end{center}
%
The remaining columns correspond to model parameters. For the
Bernoulli model, it is just the sixth column, \code{theta}. The header
line is streamed to the output file before warmup begins.

The next section describes the results of adaptation taking place during
the warmup phase.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# Adaptation terminated
# Step size = 1.81311
# Diagonal elements of inverse mass matrix:
# 0.415719
\end{Verbatim}
\end{quote}
%
The default sampler is NUTS with an adapted step size and a diagonal
inverse mass matrix.  For this example, the step size is 1.81311, and
the inverse mass contains the single entry 0.415719 corresponding to
the parameter \code{theta}.

Draws from the posterior distribution are printed out next, each line
containing a single draw with the columns corresponding to the
header.
%
\footnote{There are repeated entries due to the Metropolis accept step
in the No-U-Turn sampling algorithm.}
%
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
-6.78148,0.958918,0.997192,2,3,0,7.3034,0.283226
-6.74932,0.99923,0.997192,2,3,0,6.77915,0.243658
-6.88104,0.944956,0.997192,2,3,0,7.02671,0.317841
-6.74805,1,0.997192,2,3,0,6.84913,0.249106
-10.0366,0.49441,0.997192,2,3,0,10.1243,0.0398088
...
\end{Verbatim}
\end{quote}
%

The output ends with timing details,%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
#  Elapsed Time: 0.006811 seconds (Warm-up)
#                0.011645 seconds (Sampling)
#                0.018456 seconds (Total)
\end{Verbatim}
\end{quote}


\subsubsection{Summarizing Sampler Output}

The command-line program \code{bin/stansummary} will display summary
information about the run (for more information, see
\refchapter{stansummary}). To run \code{stansummary} on the output file
generated for \code{bernoulli} on Linux or Mac, type
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <cmdstan-home>/bin/stansummary output.csv
\end{Verbatim}
\end{quote}
%
For Windows, use backslashes to call the \code{stansummary.exe}.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <cmdstan-home>\bin\stansummary.exe output.csv
\end{Verbatim}
\end{quote}
%
The output of the command will display information about the run
followed by information for each parameter and generated quantity. For
\code{bernoulli}, we ran 1 chain and saved 1000 iterations. The
information is echoed to the standard output stream. The output is
%
\begin{Verbatim}[fontsize=\footnotesize]
Inference for Stan model: bernoulli_model
1 chains: each with iter=(1000); warmup=(0); thin=(1); 1000 iterations saved.

Warmup took (0.014) seconds, 0.014 seconds total
Sampling took (0.027) seconds, 0.027 seconds total

               Mean     MCSE   StdDev     5%   50%   95%  N_Eff  N_Eff/s    R_hat
lp__            -7.2  3.0e-02  6.6e-01   -8.5  -7.0  -6.7    479    29120  1.0e+00
accept_stat__   0.91  4.5e-03  1.4e-01   0.61  0.97   1.0   1000    60846  1.0e+00
stepsize__      1.00  3.8e-15  2.7e-15   1.00  1.00  1.00   0.50       30  1.0e+00
treedepth__      1.7  1.6e-02  4.7e-01    1.0   2.0   2.0    812    49412  1.0e+00
n_leapfrog__     2.4  3.3e-02  9.5e-01    1.0   3.0   3.0    813    49439  1.0e+00
divergent__     0.00  0.0e+00  0.0e+00   0.00  0.00  0.00   1000    60846      nan
energy__         7.7  4.6e-02  1.0e+00    6.8   7.4   9.6    491    29856  1.0e+00
theta           0.25  6.0e-03  1.1e-01  0.084  0.24  0.46    361    21985  1.0e+00

Samples were drawn using hmc with nuts.
For each parameter, N_Eff is a crude measure of effective sample size,
and R_hat is the potential scale reduction factor on split chains (at
convergence, R_hat=1).
\end{Verbatim}
%
In addition to the general information about the runs, \code{stansummary}
displays summary statistics for each parameter and generated
quantity.

In the \code{bernoulli} model, there is a single parameter,
\code{theta}. The mean, standard error of the mean, standard
deviation, the 5\%, 50\%, and 95\% quantiles, number of effective
samples (total and per second), and $\hat{R}$ value are displayed.
These quantities and their uses are described in detail in the
introductory Markov chain Monte Carlo (MCMC) chapter of the language
user's guide and reference manual.

The command \code{bin/stansummary} can be called with more than one csv file
by separating filenames with spaces. It will also take wildcards in
specifying filenames. A typical usage of Stan from the command line
would first create one or more Markov chains by calling the model
executable, typically in parallel, writing the output CSV file for
each into its own directory.  After all of the processes are
finished, the results would be analyzed using \code{stansummary} to assess
convergence and inspect the means and quantiles of the fitted
variables.  Additionally, downstream inferences may be performed using
the draws (e.g., to make decisions or predictions for unseen data).

\subsection{Optimization}

\CmdStan can be used for finding posterior modes as well as sampling
from the posterior distribution. The executable does not need to be
recompiled in order to switch from sampling to optimization, and the
data input format is the same. The following is a minimal call to
Stan's optimizer using defaults for everything but the location of the
data file. See \refsection{stan-command-line-options} for more
details.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli optimize data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
Executing this command prints the following.
%
\begin{quote}
\begin{Verbatim}[fontsize=\footnotesize]
 method = optimize
   optimize
     algorithm = lbfgs (Default)
       lbfgs
         init_alpha = 0.001 (Default)
         tol_obj = 9.9999999999999998e-13 (Default)
         tol_rel_obj = 10000 (Default)
         tol_grad = 1e-08 (Default)
         tol_rel_grad = 10000000 (Default)
         tol_param = 1e-08 (Default)
         history_size = 5 (Default)
     iter = 2000 (Default)
     save_iterations = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

initial log joint probability = -5.18908
  Iter     log prob        ||dx||      ||grad||    alpha   alpha0  # evals  Notes
     4     -5.00402    0.00400907   7.80306e-05        1        1        7
Optimization terminated normally:
  Convergence detected: relative gradient magnitude is below tolerance
\end{Verbatim}
\end{quote}
%
The first part of the output reports on the configuration used, here
indicating the default L-BFGS optimizer, with default initial stepsize
and tolerances for monitoring convergence.  The second part of the
output indicates how well the algorithm fared, here converging and
terminating normally.  The numbers reported indicate that it took 4
iterations and 7 gradient evaluations, resulting in a final state
state where the change in parameters was roughly 0.004 and the length
of the gradient roughly 8e-5.  The \code{alpha} value is for step
size used.  This is, not surprisingly, far fewer iterations than
required for sampling; even fewer iterations would be used with less
stringent user-specified convergence tolerances.


\subsubsection{Optimization Output}

The output from optimization is written into the file
\code{output.csv} by default.  The output follows the same pattern as the
output for sampling, first dumping the entire set of parameters used.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 7
# stan_version_patch = 0
# model = bernoulli_model
# method = optimize
#   optimize
#     algorithm = lbfgs (Default)
#       lbfgs
#         init_alpha = 0.001 (Default)
#         tol_obj = 9.9999999999999998e-13 (Default)
#         tol_rel_obj = 10000 (Default)
#         tol_grad = 1e-08 (Default)
#         tol_rel_grad = 10000000 (Default)
#         tol_param = 1e-08 (Default)
#         history_size = 5 (Default)
#     iter = 2000 (Default)
#     save_iterations = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 458923754
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
lp__,theta
-5.00402,0.200008
\end{Verbatim}
\end{quote}
%
Note that everything is a comment other than a line for the header,
and a line for the values.  Here, the header indicates the
unnormalized log probability with \code{lp\_\_} and the model
parameter \code{theta}.  The maximum log probability is -5.0 and the
posterior mode for \code{theta} is 0.20.  The mode exactly matches
what we would expect from the data.%
%
\footnote{The Jacobian adjustment included for the sampler's log
  probability function is not applied during optimization, because it can
  change the shape of the posterior and hence the solution.}
%
Because the prior was uniform, the result 0.20 represents the maximum
likelihood estimate (MLE) for the very simple Bernoulli model.  Note
that no uncertainty is reported.

\subsection{Variational Inference}

\CmdStan can approximate the posterior distribution using variational
inference. The executable does not need to be
recompiled in order to switch to variational inference, and the
data input format is the same. The following is a minimal call to
Stan's variational inference algorithm using defaults for everything but the
location of the data file. See \refsection{stan-command-line-options} for more
details.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli variational data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
Executing this command prints the following.
%
\begin{quote}
\begin{Verbatim}[fontsize=\footnotesize]
 method = variational
   variational
     algorithm = meanfield (Default)
       meanfield
     iter = 10000 (Default)
     grad_samples = 1 (Default)
     elbo_samples = 100 (Default)
     eta = 1 (Default)
     adapt
       engaged = 1 (Default)
       iter = 50 (Default)
     tol_rel_obj = 0.01 (Default)
     eval_elbo = 100 (Default)
     output_samples = 1000 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 1196271396
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

This is Automatic Differentiation Variational Inference.

(EXPERIMENTAL ALGORITHM: expect frequent updates to the procedure.)

Gradient evaluation took 6e-06 seconds
1000 iterations under these settings should take 0.006 seconds.
Adjust your expectations accordingly!

Begin eta adaptation.
Iteration:   1 / 250 [  0%]  (Adaptation)
Iteration:  50 / 250 [ 20%]  (Adaptation)
Iteration: 100 / 250 [ 40%]  (Adaptation)
Iteration: 150 / 250 [ 60%]  (Adaptation)
Iteration: 200 / 250 [ 80%]  (Adaptation)
Success! Found best value [eta = 1] earlier than expected.

Begin stochastic gradient ascent.
  iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes
   100         -6             1.000            1.000
   200       -6.3             0.511            1.000
   300       -6.2             0.344            0.021
   400       -6.2             0.261            0.021
   500       -6.2             0.211            0.014
   600       -6.2             0.178            0.014
   700       -6.3             0.154            0.012
   800       -6.3             0.135            0.012
   900       -6.2             0.121            0.012
  1000       -6.4             0.112            0.012
  1100       -6.3             0.015            0.012
  1200       -6.2             0.014            0.012
  1300       -6.1             0.014            0.012
  1400       -6.2             0.015            0.013
  1500       -6.3             0.015            0.012
  1600       -6.3             0.014            0.012
  1700       -6.3             0.013            0.012
  1800       -6.3             0.013            0.012
  1900       -6.2             0.013            0.013
  2000       -6.3             0.011            0.011
  2100       -6.2             0.008            0.009   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED

Drawing 1000 samples from the approximate posterior... COMPLETED.
\end{Verbatim}
\end{quote}
%
The first part of the output reports on the configuration used. Here
it indicates the default mean-field setting of the variational
inference algorithm. It also indicates the default parameter sizes and
tolerances for monitoring the algorithm's convergence. The second part
of the output describes the progression of the algorithm. An adaptation
phase finds a good value for the step size scaling parameter $\eta$. The
evidence
lower bound (ELBO) is the variational objective function and is
evaluated based on a Monte Carlo estimate. The variational inference
algorithm in Stan is stochastic, which makes it challenging to assess
convergence. That is, while the algorithm appears to have converged in
$\sim$100 iterations, the algorithm runs for another few thousand
iterations until mean change in ELBO drops below the default
tolerance of 0.01.


\subsubsection{Variational Inference Output}

The output from variational is written into the file
\code{output.csv} by default.  The output follows the same pattern as the
output for sampling, first dumping the entire set of parameters used.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 8
# stan_version_patch = 0
# model = bernoulli_model
# method = variational
#   variational
#     algorithm = meanfield (Default)
#       meanfield
#     iter = 10000 (Default)
#     grad_samples = 1 (Default)
#     elbo_samples = 100 (Default)
#     eta = 1 (Default)
#     adapt
#       engaged = 1 (Default)
#       iter = 50 (Default)
#     tol_rel_obj = 0.01 (Default)
#     eval_elbo = 100 (Default)
#     output_samples = 1000 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 1196271396
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
lp__,theta
# Stepsize adaptation complete.
# eta = 1
0,0.249604
0,0.254227
0,0.211049
...
\end{Verbatim}
\end{quote}
%
Note that everything is a comment other than a line for the header, the adapted
value for the stepsize, and a line for the values. The header indicates the
unnormalized log probability with \code{lp\_\_}. This is a legacy feature that
we do not use for variational inference. The ELBO is not stored unless a
diagnostic option is given. See \refsection{stan-command-line-options} for more
details.
The first line is special: it is the mean of the variational approximation.
The rest of the output contains \code{output\_samples} number of samples
drawn from the variational approximation.

\subsection{Configuring Command-Line Options}

The command-line options for running a model are detailed in
\refchapter{stan-cmd}. They can also be printed on the command line
using Linux or Mac OS with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli help-all
\end{Verbatim}
\end{quote}
%
and on Windows with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli.exe help-all
\end{Verbatim}
\end{quote}
%
