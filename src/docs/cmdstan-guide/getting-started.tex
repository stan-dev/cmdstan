\chapter{Getting Started}

\noindent
This chapter is designed to help users get acquanited with the
\CmdStan interface. Later chapters are devoted to expanding on the
material in this chapter with full reference documentation. See the
\Stan user's manual for details about the \Stan language.

\section{Installation}

Installation of \CmdStan is simple, just download and unpack the source.
\CmdStan requires:
\begin{itemize}
  \item The \CmdStan source and all its libraries. Everything is
    included in the release tarball or zip file.
  \item The \code{make} utility.
  \item A \Cpp compiler.
\end{itemize}

For information about supported versions of Windows, Mac, and Linux
platforms see \refappendix{install}. For step-by-step installation
instructions of the prerequisites, see
\begin{itemize}
  \item Windows: \refappendix{install-windows}
  \item Mac: \refappendix{install-mac}
  \item Linux: \refappendix{install-linux}.
\end{itemize}


\section{Building \CmdStan}

Building \CmdStan involves building executable programs for the \Stan
compiler, \code{stanc}, and a basic posterior analsyis tool,
\code{stansummary}.  The build process utilizes the \code{make} command-line
utility and works the same way across all supported platforms.
%
\begin{enumerate}
  \item Open a command-line terminal window and change directories to
    the \CmdStan directory. From here on, we'll refer to this location as
    \code{<cmdstan-home>}.
    %
    \begin{quote}
      \begin{Verbatim}[fontshape=sl]
> cd <cmdstan-home>
      \end{Verbatim}
    \end{quote}
    %
    \noindent
    A listing of the files and directory of this folder should show these files:
    %
    \begin{quote}
      \begin{Verbatim}
> ls
LICENSE            makefile
README.md          runCmdStanTests.py
doc                src
examples           stan
make               test-all.sh
      \end{Verbatim}
    \end{quote}
    %
  \item \emph{Optional:} \ Set local \code{make} variables by editing
    the file \verb+~/.config/cmdstan/make.local+ or
    \verb+<cmdstan-home>/make/local+. See \refappendix{make-options}
    for a list of available options.  For most installations, this
    step can be skipped. Setting local \code{make} variables here will
    persist between builds.
    %
  \item Use make to build \CmdStan. When multiple CPU cores are
    available on the system, the call to make can be parallelized.  It
    can either be specified directly when calling \code{make} with the
    \code{-j{\slshape N}} option, where \code{{\slshape N}} is the
    number of CPU cores. For instance, to run on 2 cores, use
    %
    \begin{quote}
      \begin{Verbatim}[fontshape=sl]
> make build -j2
      \end{Verbatim}
    \end{quote}
    %
    \emph{Warning:} \ The \code{make} program may take 10+ minutes and
    consume 2+ GB of memory to build \CmdStan.
\end{enumerate}
%
When \CmdStan is successfully built, the \code{make} program will report
%
\begin{quote}
  \begin{Verbatim}
--- CmdStan v2.9.0 built ---
  \end{Verbatim}
\end{quote}
%
and there will be two executables in the \code{bin} folder:
%
\begin{itemize}
  \item \code{stanc}, the \Stan compiler. The \Stan compiler
    translates a \Stan program into \Cpp code. See \refchapter{stanc}
    for details.
  \item \code{stansummary}, a posterior analysis tool. The \code{stansummary}
    command summarizes the comma-separated values files that are
    generated from \Stan program runs. For each parameter within the
    \Stan program, \code{stansummary} reports the mean, standard deviation,
    quantiles, $\hat{R}$, and other values. See \refchapter{stansummary} for
    details.
\end{itemize}

\section{Compiling and Executing a Stan Program}\label{compiling-model.section}

The rest of this quick-start guide explains how to code and run a very
simple Bayesian model.

\subsection{A Simple Bernoulli Model}

The following is a simple, complete Stan program for a Bernoulli model
of binary data.%
%
\footnote{The model is distributed with Stan's example models
  repository,
  \url{https://github.com/stan-dev/example-models/blob/master/basic_estimators/bernoulli.stan},
  and is also available with the CmdStan distribution at the path
  \nolinkurl{examples/bernoulli/bernoulli.stan}.  }
%
\begin{quote}
\begin{Verbatim}
data {
  int<lower=0> N;
  int<lower=0,upper=1> y[N];
}
parameters {
  real<lower=0,upper=1> theta;
}
model {
  theta ~ beta(1,1);
  for (n in 1:N)
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
The model assumes the binary observed data \code{y[1],...,y[N]}
are i.i.d.\ with Bernoulli chance-of-success \code{theta}.  The
prior on \code{theta} is \code{beta(1,1)} (i.e., uniform).

\subsection{Data Set}

A data set of $\mbox{\code{N}}=10$ observations is coded as follows.
%
\footnote{
The data is also available from the \code{example-models} repository
on GitHub,
\url{https://github.com/stan-dev/example-models/blob/master/basic_estimators/bernoulli.data.R}.
It is also included with the CmdStan distribution and can be found at path
\nolinkurl{stan/example-models/basic_estimators/bernoulli.data.R}.
}
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
This defines the contents of two variables, \code{N} and \code{y},
using an R-like syntax (see \refchapter{dump} for more information).

\subsection{Change directories to \code{<cmdstan-home>}}

Before building any Stan program, change directories to \code{<cmdstan-home>}.

\subsection{Compiling a \Stan Program}

A single call to \code{make} will translate a \Stan program to \Cpp,
then compile the \Cpp code to an executable for the command line.

A \Stan program must be in a file with the file extension
\code{.stan}. To create an executable from a \Stan program,
\code{make} will be called with the name of the executable as its
argument. For Mac and Linux, it is the name of the \Stan program with
the \code{.stan} omitted. For Windows, replace \code{.stan} with
\code{.exe}.

To build the Bernoulli example, use
the following command for Mac and Linux:
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make examples/bernoulli/bernoulli
\end{Verbatim}
\end{quote}
%
For Windows, the command is the same with the addition of \code{.exe}
at the end of the target:
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make examples/bernoulli/bernoulli.exe
\end{Verbatim}
\end{quote}

The generated \Cpp code (\code{bernoulli.hpp}) and the compiled
executable will be placed in the same directory as the \Stan program.

\emph{Note:} to build a \Stan program, you must be in the
\code{<cmdstan-home>} directory. The path to the \Stan program must not
contain a space. If using relative paths, the relative path must not
contain a space.

\subsection{Sampling from the \Stan Program}

The program can be executed from the directory in which it resides.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd examples/bernoulli
\end{Verbatim}
\end{quote}
%
To execute sampling of the model under Linux or Mac, use
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
In Windows, the \code{./} prefix is not needed, resulting in the
following command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli.exe sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
The output is the same across all supported platforms.  First, the
configuration of the program is echoed to the standard output:
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
 method = sample (Default)
   sample
     num_samples = 1000 (Default)
     num_warmup = 1000 (Default)
     save_warmup = 0 (Default)
     thin = 1 (Default)
     adapt
       engaged = 1 (Default)
       gamma = 0.050000000000000003 (Default)
       delta = 0.80000000000000004 (Default)
       kappa = 0.75 (Default)
       t0 = 10 (Default)
       init_buffer = 75 (Default)
       term_buffer = 50 (Default)
       window = 25 (Default)
     algorithm = hmc (Default)
       hmc
         engine = nuts (Default)
           nuts
             max_depth = 10 (Default)
         metric = diag_e (Default)
         stepsize = 1 (Default)
         stepsize_jitter = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)
\end{Verbatim}
\end{quote}
%
After the configuration has been displayed, a short timing message is
given.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
Gradient evaluation took 4e-06 seconds
1000 transitions using 10 leapfrog steps per transition would
  take 0.04 seconds.
Adjust your expectations accordingly!
\end{Verbatim}
\end{quote}
%
Next, the sampler reports the iteration number, reporting the
percentage complete.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:  100 / 2000 [  5%]  (Warmup)
...
Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}

\subsubsection{Sampler Output}

Each execution of the model results in draws from a single Markov
chain being written to a file in comma-separated value (CSV) format.
The default name of the output file is \nolinkurl{output.csv}.

The first part of the output file records the version of the
underlying \Stan library and the configuration as comments (i.e.,
lines beginning with the pound sign (\Verb|#|)).
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 8
# stan_version_patch = 0
# model = bernoulli_model
# method = sample (Default)
#   sample
#     num_samples = 1000 (Default)
#     num_warmup = 1000 (Default)
#     save_warmup = 0 (Default)
#     thin = 1 (Default)
...
\end{Verbatim}
\end{quote}
%
This is followed by a CSV header indicating the names of the values
sampled.
%
\begin{Verbatim}[fontsize=\small]
lp__,accept_stat__,stepsize__,treedepth__,n_leapfrog__,n_divergent__,theta
\end{Verbatim}
%
The first column reports the unnormalized log probability of the
model.  The next columns provide sampler-dependent information; here,
it is columns two through five. For basic Hamiltonian Monte Carlo
(HMC) and its adaptive variant, the No-U-Turn sampler (NUTS), the
sampler-depedent parameters are described in the following table.
%
\begin{center}
\begin{tabular}{l|l|l}
{\it Sampler} & {\it Parameter} & {\it Description}
\\ \hline \hline
\NUTS & \Verb| accept_stat__  | & Metropolis acceptance probability
\\
& & averaged over samples in the slice
\\
\NUTS & \Verb| stepsize__ | & Integrator step size
\\
\NUTS & \Verb| treedepth__  | & Tree depth
\\
\NUTS & \Verb| n_leapfrog__  | & Number of leapfrog calculations
\\
\NUTS & \Verb| n_divergent__  | & Number of divergent iterations
\\ \hline
\HMC & \Verb| accept_stat__ | &  Metropolis acceptance probability
\\
\HMC & \Verb| stepsize__  | & Integrator step size
\\
\HMC & \Verb| int_time__  | & Total integration time
\\
\end{tabular}
\end{center}
%
The remaining columns correspond to model parameters. For the
Bernoulli model, it is just the sixth column, \code{theta}. The header
line is streamed to the output file before warmup begins.

The next section describes the results of adaptation taking place during
the warmup phase.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# Adaptation terminated
# Step size = 1.81311
# Diagonal elements of inverse mass matrix:
# 0.415719
\end{Verbatim}
\end{quote}
%
The default sampler is NUTS with an adapted step size and a diagonal
inverse mass matrix.  For this example, the step size is 1.81311, and
the inverse mass contains the single entry 0.415719 corresponding to
the parameter \code{theta}.

Draws from the posterior distribution are printed out next, each line
containing a single draw with the columns corresponding to the
header.
%
\footnote{There are repeated entries due to the Metropolis accept step
in the No-U-Turn sampling algorithm.}
%
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
-7.22574,1,1.09068,1,1,0.383089
-6.95293,0.945991,1.09068,2,3,0.335074
-6.92373,0.938744,1.09068,1,1,0.181194
-6.83655,0.934833,1.09068,2,3,0.304882
-7.01732,1,1.09068,1,1,0.348244
...
\end{Verbatim}
\end{quote}
%

The output ends with timing details,%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
#  Elapsed Time: 0.006811 seconds (Warm-up)
#                0.011645 seconds (Sampling)
#                0.018456 seconds (Total)
\end{Verbatim}
\end{quote}


\subsubsection{Summarizing Sampler Output}

The command-line program \code{bin/stansummary} will display summary
information about the run (for more information, see
\refchapter{stansummary}). To run \code{stansummary} on the output file
generated for \code{bernoulli} on Linux or Mac, type
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <cmdstan-home>/bin/stansummary output.csv
\end{Verbatim}
\end{quote}
%
For Windows, use backslashes to call the \code{stansummary.exe}.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <cmdstan-home>\bin\stansummary.exe output.csv
\end{Verbatim}
\end{quote}
%
The output of the command will display information about the run
followed by information for each parameter and generated quantity. For
\code{bernoulli}, we ran 1 chain and saved 1000 iterations. The
information is echoed to the standard output stream. The output is
%
\begin{Verbatim}[fontsize=\footnotesize]
Inference for Stan model: bernoulli_model
1 chains: each with iter=(1000); warmup=(0); thin=(1); 1000 iterations saved.

Warmup took (0.014) seconds, 0.014 seconds total
Sampling took (0.027) seconds, 0.027 seconds total

                Mean     MCSE   StdDev     5%   50%   95%  N_Eff  N_Eff/s    R_hat
lp__            -7.3  4.6e-02  7.4e-01   -8.7  -7.0  -6.8    255     9267  1.0e+00
accept_stat__   0.94  2.7e-03  8.5e-02   0.76  0.98   1.0   1000    36395  1.0e+00
stepsize__      0.79  1.1e-15  7.8e-16   0.79  0.79  0.79   0.50       18  1.0e+00
treedepth__      1.9  2.2e-02  6.5e-01    1.0   2.0   3.0    844    30733  1.0e+00
n_leapfrog__     3.1  6.4e-02  1.9e+00    1.0   3.0   7.0    897    32644  1.0e+00
n_divergent__   0.00  0.0e+00  0.0e+00   0.00  0.00  0.00   1000    36395      nan
theta           0.26  5.9e-03  1.2e-01  0.091  0.25  0.48    420    15292  1.0e+00

Samples were drawn using hmc with nuts.
For each parameter, N_Eff is a crude measure of effective sample size,
and R_hat is the potential scale reduction factor on split chains (at
convergence, R_hat=1).
\end{Verbatim}
%
In addition to the general information about the runs, \code{stansummary}
displays summary statistics for each parameter and generated
quantity.

In the \code{bernoulli} model, there is a single parameter,
\code{theta}. The mean, standard error of the mean, standard
deviation, the 5\%, 50\%, and 95\% quantiles, number of effective
samples (total and per second), and $\hat{R}$ value are displayed.
These quantities and their uses are described in detail in the
introductory Markov chain Monte Carlo (MCMC) chapter of the language
user's guide and reference manual.

The command \code{bin/stansummary} can be called with more than one csv file
by separating filenames with spaces. It will also take wildcards in
specifying filenames. A typical usage of Stan from the command line
would first create one or more Markov chains by calling the model
executable, typically in parallel, writing the output CSV file for
each into its own directory.  After all of the processes are
finished, the results would be analyzed using \code{stansummary} to assess
convergence and inspect the means and quantiles of the fitted
variables.  Additionally, downstream inferences may be performed using
the draws (e.g., to make decisions or predictions for unseen data).

\subsection{Optimization}

\CmdStan can be used for finding posterior modes as well as sampling
from the posterior distribution. The executable does not need to be
recompiled in order to switch from sampling to optimization, and the
data input format is the same. The following is a minimal call to
\Stan's optimizer using defaults for everything but the location of the
data file. See \refsection{stan-command-line-options} for more
details.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli optimize data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
Executing this command prints the following.
%
\begin{quote}
\begin{Verbatim}[fontsize=\footnotesize]
 method = optimize
   optimize
     algorithm = lbfgs (Default)
       lbfgs
         init_alpha = 0.001 (Default)
         tol_obj = 9.9999999999999998e-13 (Default)
         tol_rel_obj = 10000 (Default)
         tol_grad = 1e-08 (Default)
         tol_rel_grad = 10000000 (Default)
         tol_param = 1e-08 (Default)
         history_size = 5 (Default)
     iter = 2000 (Default)
     save_iterations = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

initial log joint probability = -5.18908
  Iter     log prob        ||dx||      ||grad||    alpha   alpha0  # evals  Notes
     4     -5.00402    0.00400907   7.80306e-05        1        1        7
Optimization terminated normally:
  Convergence detected: relative gradient magnitude is below tolerance
\end{Verbatim}
\end{quote}
%
The first part of the output reports on the configuration used, here
indicating the default L-BFGS optimizer, with default initial stepsize
and tolerances for monitoring convergence.  The second part of the
output indicates how well the algorithm fared, here converging and
terminating normally.  The numbers reported indicate that it took 4
iterations and 7 gradient evaluations, resulting in a final state
state where the change in parameters was roughly 0.004 and the length
of the gradient roughly 8e-5.  The \code{alpha} value is for step
size used.  This is, not surprisingly, far fewer iterations than
required for sampling; even fewer iterations would be used with less
stringent user-specified convergence tolerances.


\subsubsection{Optimization Output}

The output from optimization is written into the file
\code{output.csv} by default.  The output follows the same pattern as the
output for sampling, first dumping the entire set of parameters used.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 7
# stan_version_patch = 0
# model = bernoulli_model
# method = optimize
#   optimize
#     algorithm = lbfgs (Default)
#       lbfgs
#         init_alpha = 0.001 (Default)
#         tol_obj = 9.9999999999999998e-13 (Default)
#         tol_rel_obj = 10000 (Default)
#         tol_grad = 1e-08 (Default)
#         tol_rel_grad = 10000000 (Default)
#         tol_param = 1e-08 (Default)
#         history_size = 5 (Default)
#     iter = 2000 (Default)
#     save_iterations = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 458923754
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
lp__,theta
-5.00402,0.200008
\end{Verbatim}
\end{quote}
%
Note that everything is a comment other than a line for the header,
and a line for the values.  Here, the header indicates the
unnormalized log probability with \code{lp\_\_} and the model
parameter \code{theta}.  The maximum log probability is -5.0 and the
posterior mode for \code{theta} is 0.20.  The mode exactly matches
what we would expect from the data.%
%
\footnote{The Jacobian adjustment included for the sampler's log
  probability function is not applied during optimization, because it can
  change the shape of the posterior and hence the solution.}
%
Because the prior was uniform, the result 0.20 represents the maximum
likelihood estimate (MLE) for the very simple Bernoulli model.  Note
that no uncertainty is reported.

\subsection{Variational Inference}

\CmdStan can approximate the posterior distribution using variational
inference. The executable does not need to be
recompiled in order to switch to variational inference, and the
data input format is the same. The following is a minimal call to
\Stan's variational inference algorithm using defaults for everything but the
location of the data file. See \refsection{stan-command-line-options} for more
details.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli variational data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
Executing this command prints the following.
%
\begin{quote}
\begin{Verbatim}[fontsize=\footnotesize]
 method = variational
   variational
     algorithm = meanfield (Default)
       meanfield
     iter = 10000 (Default)
     grad_samples = 1 (Default)
     elbo_samples = 100 (Default)
     eta = 1 (Default)
     adapt
       engaged = 1 (Default)
       iter = 50 (Default)
     tol_rel_obj = 0.01 (Default)
     eval_elbo = 100 (Default)
     output_samples = 1000 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 1196271396
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

This is Automatic Differentiation Variational Inference.

(EXPERIMENTAL ALGORITHM: expect frequent updates to the procedure.)

Gradient evaluation took 6e-06 seconds
1000 iterations under these settings should take 0.006 seconds.
Adjust your expectations accordingly!

Begin eta adaptation.
Iteration:   1 / 250 [  0%]  (Adaptation)
Iteration:  50 / 250 [ 20%]  (Adaptation)
Iteration: 100 / 250 [ 40%]  (Adaptation)
Iteration: 150 / 250 [ 60%]  (Adaptation)
Iteration: 200 / 250 [ 80%]  (Adaptation)
Success! Found best value [eta = 1] earlier than expected.

Begin stochastic gradient ascent.
  iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes
   100         -6             1.000            1.000
   200       -6.3             0.511            1.000
   300       -6.2             0.344            0.021
   400       -6.2             0.261            0.021
   500       -6.2             0.211            0.014
   600       -6.2             0.178            0.014
   700       -6.3             0.154            0.012
   800       -6.3             0.135            0.012
   900       -6.2             0.121            0.012
  1000       -6.4             0.112            0.012
  1100       -6.3             0.015            0.012
  1200       -6.2             0.014            0.012
  1300       -6.1             0.014            0.012
  1400       -6.2             0.015            0.013
  1500       -6.3             0.015            0.012
  1600       -6.3             0.014            0.012
  1700       -6.3             0.013            0.012
  1800       -6.3             0.013            0.012
  1900       -6.2             0.013            0.013
  2000       -6.3             0.011            0.011
  2100       -6.2             0.008            0.009   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED

Drawing 1000 samples from the approximate posterior... COMPLETED.
\end{Verbatim}
\end{quote}
%
The first part of the output reports on the configuration used. Here
it indicates the default mean-field setting of the variational
inference algorithm. It also indicates the default parameter sizes and
tolerances for monitoring the algorithm's convergence. The second part
of the output describes the progression of the algorithm. An adaptation
phase finds a good value for the step size scaling parameter $\eta$. The
evidence
lower bound (ELBO) is the variational objective function and is
evaluated based on a Monte Carlo estimate. The variational inference
algorithm in \Stan is stochastic, which makes it challenging to assess
convergence. That is, while the algorithm appears to have converged in
$\sim$100 iterations, the algorithm runs for another few thousand
iterations until mean change in ELBO drops below the default
tolerance of 0.01.


\subsubsection{Variational Inference Output}

The output from variational is written into the file
\code{output.csv} by default.  The output follows the same pattern as the
output for sampling, first dumping the entire set of parameters used.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 8
# stan_version_patch = 0
# model = bernoulli_model
# method = variational
#   variational
#     algorithm = meanfield (Default)
#       meanfield
#     iter = 10000 (Default)
#     grad_samples = 1 (Default)
#     elbo_samples = 100 (Default)
#     eta = 1 (Default)
#     adapt
#       engaged = 1 (Default)
#       iter = 50 (Default)
#     tol_rel_obj = 0.01 (Default)
#     eval_elbo = 100 (Default)
#     output_samples = 1000 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 1196271396
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
lp__,theta
# Stepsize adaptation complete.
# eta = 1
0,0.249604
0,0.254227
0,0.211049
...
\end{Verbatim}
\end{quote}
%
Note that everything is a comment other than a line for the header, the adapted
value for the stepsize, and a line for the values. The header indicates the
unnormalized log probability with \code{lp\_\_}. This is a legacy feature that
we do not use for variational inference. The ELBO is not stored unless a
diagnostic option is given. See \refsection{stan-command-line-options} for more
details.
The first line is special: it is the mean of the variational approximation.
The rest of the output contains \code{output\_samples} number of samples
drawn from the variational approximation.

\subsection{Configuring Command-Line Options}

The command-line options for running a model are detailed in
\refchapter{stan-cmd}. They can also be printed on the command line
using Linux or Mac OS with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli help-all
\end{Verbatim}
\end{quote}
%
and on Windows with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli.exe help-all
\end{Verbatim}
\end{quote}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \chapter{--- sections cut out of this chapter ---}
%% \section{For BUGS Users}

%% An appendix in the language user's guide and reference manual
%% describes some similarities and important differences between Stan and
%% BUGS (including WinBUGS, OpenBUGs, and JAGS).




%% \subsubsection{Implicit Uniform Priors}

%% If no prior is specified for a parameter, it is implicitly given a
%% uniform prior on its support. For parameters such as \code{theta} in
%% the example, which are constrained to fall between 0 and 1, this
%% produces a proper uniform distribution on the support of \code{theta}.
%% Because $\distro{Beta}(1,1)$ is the uniform distribution, the
%% following sampling statement can be eliminated from the model without
%% changing the log probability calculation.
%% %
%% \begin{quote}
%% \begin{Verbatim}
%%   theta ~ beta(1,1);
%% \end{Verbatim}
%% \end{quote}

%% For parameters with unbounded support, the implicit uniform prior is
%% improper.  Stan allows improper priors to be specified in models, but
%% posteriors must be proper in order for sampling to succeed.


%% \subsubsection{Constraints on Parameters}

%% The variable \code{theta} is defined with lower and upper bounds,
%% which constrain its value.  Parameters with constrained support should
%% always specify appropriate constraints in the parameter declaration;
%% if the constraints are absent, sampling will either slow down or stop
%% altogether based on whether the initial values satisfy the constraints.

%% \subsubsection{Vectorizing Sampling Statements}

%% Iterations of the model will be faster if the loop over sampling
%% statements is \textit{vectorized} by replacing
%% %
%% \begin{quote}
%% \begin{Verbatim}
%%   for (n in 1:N)
%%     y[n] ~ bernoulli(theta);
%% \end{Verbatim}
%% \end{quote}
%% %
%% with the equivalent vectorized form,
%% %
%% \begin{quote}
%% \begin{Verbatim}
%%   y ~ bernoulli(theta);
%% \end{Verbatim}
%% \end{quote}
%% %
%% Performance gains from vectorization are not because loops are slow in
%% Stan, but because calls to sampling statements are slow.
%% Vectorization allows multiple calls to a sampling statement to be
%% replaced with a single call that can share common calculations for the
%% log probability function, its gradients, and error checking.  For more
%% tips on optimizing the performance of Stan models, see the chapter in
%% the language user's guide and reference manual on optimization.



%% \subsection{Testing Stan}\label{testing-stan.section}

%% To run the Stan unit tests of basic functionality, run the following
%% commands from a shell (where \code{<stan-home>} is replaced top-level
%% directory into which Stan was unpacked; it should contain a file named
%% \code{makefile}).%
%% %
%% \footnote{Although command-line Stan runs with earlier versions of
%%   \code{make}, the unit tests require version 3.81 or higher; see
%%   \refsection{windows-make-install} for installation instructions.}
%% %
%% \begin{quote}
%% \begin{Verbatim}[fontshape=sl]
%% > cd <stan-home>
%% > make -j4 O=0 test-headers
%% > make -j4 O=0 src/test/unit
%% > make -j4 O=0 src/test/unit-agrad-rev
%% > make -j4 O=0 src/test/unit-agrad-fwd
%% > make -j4 O=0 src/test/unit-distribution
%% > make -j4 O=3 src/test/CmdStan/models
%% \end{Verbatim}
%% \end{quote}
%% %
%% As before, \code{-j4} indicates that four processes should be run in
%% parallel; adjust the value \code{4} to correspond to the number of CPU
%% cores available. Code optimization is specified by the letter `O'
%% followed by an equal sign followed by the digit `0' for no
%% optimization and `3' for more optimization; optimization slows down
%% compilation of the executable but reduces its execution time.
%% Warnings can be safely ignored if the tests complete without a
%% \code{FAIL} error.

%% \emph{Warning}: \ The unit tests can take 30+ minutes and consume 3+
%% GB of memory with the default compiler, \code{g++}.  The distribution
%% test and model tests can take even longer.  It is faster to run the
%% Clang compiler (option \code{CC=clang++}), and to run in multiple
%% processes in parallel (e.g., option \code{-j4} for four threads).

%% \subsection{Compile-time Errors}

%% \Stan tries to report errors and warnings in order to help users
%% correctly formulate and debug models. Compile-time errors occur when
%% \Stan programs are not syntaxtically valid. These errors prevent \Cpp
%% code from being generated and must be addressed before proceeding. See
%% the user's guide for more details.

%% \subsection{Run-time Warnings}

%% Run-time warnings occur while the program is executing.

%% \subsubsection{Metropolis Rejection Warning}

%% The Metropolis sampler may reject a proposal due to an error arising
%% in the evaluation of the log probability function.  Such errors are
%% typically due to underflow or overflow in numerical computations that
%% are the unavoidable consequence of floating-point approximations to
%% continuous real values.  The following is an example of such a
%% message.
%% %
%% \begin{quote}\small
%% \begin{Verbatim}
%% Informational Message: The current Metropolis proposal is about
%% to be rejected because of the following issue:
%%   stan::prob::normal_log(d): Scale parameter is 0, but must be > 0!
%% If this warning occurs sporadically, such as for highly constrained
%% variable types like covariance matrices, then the sampler is fine,
%% but if this warning occurs often then your model may be either severely
%% ill-conditioned or misspecified.
%% \end{Verbatim}
%% \end{quote}
%% %
%% This is just an \emph{informational message} \, (i.e., a warning); if
%% the message only occurs sporadically, then {\it the sampler is fine}
%% and the user need not worry.  Particularly in early stages of sampler
%% adaptation before adaptation has converged on the high-mass volume of
%% the posterior, the numerical approximation to functions and to the
%% Hamiltonian dynamics followed by the sampler can lead to numerical
%% issues. The post-adaptation draws are still valid with the presence of
%% these messages.

%% If this message occurs repeatedly, the model may have a poorly
%% conditioned constraint (typically with covariance or correlation
%% matrices) or may be misformulated.  In a future version of \Stan, we
%% plan to rank such messages by severity and turn this one off by
%% default so as not to needlessly worry users.

